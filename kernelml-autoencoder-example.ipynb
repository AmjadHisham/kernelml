{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing numpy on engine(s)\n",
      "importing stats from scipy on engine(s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohankotwani/anaconda/envs/datasci/lib/python3.5/site-packages/ipyparallel/client/client.py:442: RuntimeWarning: \n",
      "            Controller appears to be listening on localhost, but not on this machine.\n",
      "            If this is true, you should specify Client(...,sshserver='you@192.168.1.3')\n",
      "            or instruct your controller to listen on an external IP.\n",
      "  RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import seaborn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "import kernelml\n",
    "import re\n",
    "\n",
    "from ipyparallel import Client\n",
    "rc = Client(profile='default')\n",
    "dview = rc[:]\n",
    "\n",
    "dview.block = True\n",
    "\n",
    "with dview.sync_imports():\n",
    "    import numpy as np\n",
    "    from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full = pd.read_csv('DATA/hb_training.csv')\n",
    "test = pd.read_csv('DATA/hb_testing.csv')\n",
    "\n",
    "def change_label(x):\n",
    "    if x =='s':\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "full['Label'] = full['Label'].apply(change_label)\n",
    "EventId = test['EventId']\n",
    "full.drop(['EventId'],axis=1,inplace=True)\n",
    "test.drop(['EventId'],axis=1,inplace=True)\n",
    "features = list(full.columns[:-2])\n",
    "target = list(full.columns[-1:])\n",
    "\n",
    "all_samples=full.index\n",
    "ones = full[full[target].values==1].index\n",
    "zeros = full[full[target].values==0].index\n",
    "ones_rand_sample = np.random.choice(ones, size=int(len(ones)*0.5),replace=False)\n",
    "zeros_rand_sample = np.random.choice(zeros, size=int(len(zeros)*0.5),replace=False)\n",
    "rand_sample  = np.concatenate((ones_rand_sample,zeros_rand_sample))\n",
    "np.random.shuffle(rand_sample)\n",
    "\n",
    "test_sample = np.setdiff1d(all_samples,rand_sample)\n",
    "valid = full.loc[test_sample,:]\n",
    "train = full.loc[rand_sample,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNShapeHelper():\n",
    "\n",
    "    def __init__(self,layer_shape,num_inputs,num_outputs):\n",
    "        \n",
    "        self.N_inputs = num_inputs\n",
    "        self.N_outputs = num_outputs\n",
    "        self.layer_shape = layer_shape\n",
    "        self.N_layers = len(layer_shape)\n",
    "        self.model_shape = []\n",
    "        self.parameter_shape = []\n",
    "        \n",
    "    def get_N_parameters(self):\n",
    "        \n",
    "        self.model_shape.append(self.N_inputs)\n",
    "        input_n_parameters = self.N_inputs*self.layer_shape[0]\n",
    "        N =  input_n_parameters\n",
    "        self.parameter_shape.append(input_n_parameters)\n",
    "        \n",
    "        for i in range(1,self.N_layers):\n",
    "            layer_n_parameters = self.layer_shape[i-1]*self.layer_shape[i]\n",
    "            self.model_shape.append(self.layer_shape[i])\n",
    "            self.parameter_shape.append(layer_n_parameters)\n",
    "            N += layer_n_parameters\n",
    "            \n",
    "        output_n_parameters = self.N_outputs*self.layer_shape[-1]\n",
    "        N += output_n_parameters\n",
    "        self.model_shape.append(self.N_outputs)\n",
    "        self.parameter_shape.append(output_n_parameters)\n",
    "        self.N_parameters = N\n",
    "        return N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[30, 30, 1], [300, 300, 30]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapehelper = NNShapeHelper([10,len(features)],len(features),1)\n",
    "num_parameters = shapehelper.get_N_parameters()\n",
    "shapes = [shapehelper.model_shape,shapehelper.parameter_shape]\n",
    "shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def autoencoder_function(X,y,w_tensor,args,predict=False):\n",
    "    from scipy import stats\n",
    "    import numpy as np\n",
    "\n",
    "    model_shape,parameter_shape,link = args\n",
    "    # define the loss function between predicted output actual output\n",
    "    def nn_autoencoder_loss(hypothesis,y):\n",
    "        return np.sum((hypothesis-y)**2)/y.size\n",
    "\n",
    "    def reshape_vector(w):\n",
    "        reshape_w = []\n",
    "        indx = 0\n",
    "        for shape,num in zip(model_shape,parameter_shape):\n",
    "            x = w[indx:num+indx]\n",
    "            if x.size!=num:\n",
    "                continue\n",
    "            x = x.reshape(shape,int(num/shape))\n",
    "            reshape_w.append(x)\n",
    "            indx = indx+num\n",
    "        extra_w = w[indx:]\n",
    "        return reshape_w,extra_w\n",
    "        \n",
    "    #Specifies the way the tensors are combined with the inputs\n",
    "    def combine_tensors(X,w_tensor,link):\n",
    "        w_tensor,extra_w = reshape_vector(w_tensor)\n",
    "        b1,a1,b2,a2 = extra_w[:4]\n",
    "        pred = X.dot(w_tensor[0])\n",
    "        if link == 'linear':\n",
    "            pred = a1*(pred+b1)\n",
    "        elif link == 'field_eq':\n",
    "            pred = -0.5*a1*pred + b1*pred\n",
    "        elif link == 'log':\n",
    "            pred = a1*pred + np.log(np.abs(pred)+b1)\n",
    "        elif link == 'inverse':\n",
    "            pred = a1/(pred+b1)\n",
    "\n",
    "        pred = pred.dot(w_tensor[1].T)\n",
    "        pred = a2*(pred+b2)\n",
    "        return pred\n",
    "\n",
    "    #we cannot modify pickled memory so create a copy of the parameter vector\n",
    "    w_tensor_copy = w_tensor.copy()\n",
    "    pred = combine_tensors(X,w_tensor_copy,link)\n",
    "    if predict==True:\n",
    "        return pred\n",
    "    loss = nn_autoencoder_loss(pred,y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('run', 0, 'loss', 74127.473644495651, 'time', 5.297924041748047)\n",
      "('run', 1, 'loss', 50078.598234739242, 'time', 6.78652811050415)\n",
      "('run', 2, 'loss', 39509.535970519275, 'time', 7.27153205871582)\n",
      "('run', 3, 'loss', 28246.028423822569, 'time', 7.761844158172607)\n",
      "('run', 4, 'loss', 20813.586045332457, 'time', 6.9654059410095215)\n",
      "('run', 5, 'loss', 20011.956343675116, 'time', 6.292503118515015)\n",
      "('run', 6, 'loss', 16187.0434892143, 'time', 7.679632902145386)\n",
      "('run', 7, 'loss', 13276.427739302409, 'time', 6.465327024459839)\n",
      "('run', 8, 'loss', 13606.149401214321, 'time', 7.927728891372681)\n",
      "('run', 9, 'loss', 11464.873209093155, 'time', 7.522006988525391)\n",
      "('run', 10, 'loss', 10167.015141196156, 'time', 7.31041693687439)\n",
      "('run', 11, 'loss', 9712.0885036865202, 'time', 6.454974174499512)\n",
      "('run', 12, 'loss', 8216.8781808712229, 'time', 6.970394849777222)\n",
      "('run', 13, 'loss', 9273.3453231401418, 'time', 10.173343896865845)\n",
      "('run', 14, 'loss', 8354.439366233717, 'time', 6.293745994567871)\n",
      "('run', 15, 'loss', 7271.4412389170311, 'time', 8.43854284286499)\n",
      "('run', 16, 'loss', 7666.6860902844846, 'time', 8.73488998413086)\n",
      "('run', 17, 'loss', 6733.5765356916845, 'time', 8.387165784835815)\n",
      "('run', 18, 'loss', 7489.1558919681611, 'time', 7.345221042633057)\n",
      "('run', 19, 'loss', 6393.4990026264632, 'time', 8.411450862884521)\n"
     ]
    }
   ],
   "source": [
    "X = train[features].values\n",
    "y = train[target].values\n",
    "\n",
    "#prior parameter sampler (default)\n",
    "def prior_sampler_uniform_distribution(weights,num_param):\n",
    "    return np.random.uniform(low=-1,high=1,size=(num_param,1000))\n",
    "\n",
    "#sampler function (default)\n",
    "def sampler_multivariate_normal_distribution(best_param,\n",
    "                                            param_by_iter,\n",
    "                                            error_by_iter,\n",
    "                                            parameter_update_history,\n",
    "                                            random_sample_num=100):\n",
    "    covariance = np.diag(np.var(parameter_update_history[:,:],axis=1))\n",
    "    best = param_by_iter[np.where(error_by_iter==np.min(error_by_iter))[0]]\n",
    "    mean = best.flatten()\n",
    "    try:\n",
    "        return np.random.multivariate_normal(mean, covariance, (random_sample_num)).T\n",
    "    except:\n",
    "        print(best,np.where(error_by_iter==np.min(error_by_iter)))\n",
    "\n",
    "#intermediate sampler\n",
    "def intermediate_uniform_distribution(weights,num_param):\n",
    "    result = []\n",
    "    for i in range(num_param):\n",
    "        x = np.random.uniform(weights[i]-0.1*weights[i],weights[i]+0.1*weights[i],size=(1,10000)).T\n",
    "        result.append(x)\n",
    "    result = np.squeeze(np.array(result))\n",
    "    return result          \n",
    "\n",
    "#parameter transform\n",
    "def positive_int_transform(w):\n",
    "    out = w.copy()\n",
    "    extra_w = out[-4:]\n",
    "    for i in range(extra_w.shape[0]):\n",
    "        extra_w[i][np.where(extra_w[i]<=0)[0]]=1e-6\n",
    "    out[-4:] = extra_w\n",
    "    return out.reshape(w.shape)\n",
    "\n",
    "runs=20\n",
    "zscore = 2.0\n",
    "umagnitude = 0.00001\n",
    "analyzenparam = 100\n",
    "nupdates = 1\n",
    "npriorsamples=3600\n",
    "nrandomsamples = 2400\n",
    "tinterations = 5\n",
    "sequpdate = False\n",
    "link = 'field_eq'\n",
    "\n",
    "kml = kernelml.KernelML(\n",
    "         prior_sampler_fcn=None,\n",
    "         sampler_fcn=None,\n",
    "         intermediate_sampler_fcn=None,\n",
    "         parameter_transform_fcn=positive_int_transform,\n",
    "         batch_size=500)\n",
    "\n",
    "kml.use_ipyparallel(dview)\n",
    "\n",
    "parameter_by_run = kml.optimize(X,X,loss_function=autoencoder_function,\n",
    "                                num_param=num_parameters+4,\n",
    "                                args=shapes+[link],\n",
    "                                runs=runs,\n",
    "                                total_iterations=tinterations,\n",
    "                                analyze_n_parameters=analyzenparam,\n",
    "                                n_parameter_updates=nupdates,\n",
    "                                update_magnitude=umagnitude,\n",
    "                                sequential_update=sequpdate,\n",
    "                                percent_of_params_updated=0.8,\n",
    "                                init_random_sample_num=npriorsamples,\n",
    "                                random_sample_num=nrandomsamples,\n",
    "                                prior_uniform_low=-1,\n",
    "                                prior_uniform_high=1,\n",
    "                                convergence_z_score=1,\n",
    "                                plot_feedback=False,\n",
    "                                print_feedback=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance by run\n",
      "iteration 0 train rsquared 0.180410618791 test rsquared 0.17804373913\n",
      "iteration 1 train rsquared 0.442575202917 test rsquared 0.441883042778\n",
      "iteration 2 train rsquared 0.576914430707 test rsquared 0.577318067865\n",
      "iteration 3 train rsquared 0.687203870348 test rsquared 0.687219718117\n",
      "iteration 4 train rsquared 0.755861512033 test rsquared 0.755825561273\n",
      "iteration 5 train rsquared 0.782444293165 test rsquared 0.781962506948\n",
      "iteration 6 train rsquared 0.80687445835 test rsquared 0.806645611277\n",
      "iteration 7 train rsquared 0.837667248039 test rsquared 0.837267645517\n",
      "iteration 8 train rsquared 0.853708626912 test rsquared 0.853180227232\n",
      "iteration 9 train rsquared 0.872960231473 test rsquared 0.872410102747\n",
      "iteration 10 train rsquared 0.882847399003 test rsquared 0.882314972285\n",
      "iteration 11 train rsquared 0.891115458848 test rsquared 0.890754462165\n",
      "iteration 12 train rsquared 0.899594302523 test rsquared 0.899149195401\n",
      "iteration 13 train rsquared 0.904035964854 test rsquared 0.903664274275\n",
      "iteration 14 train rsquared 0.907649417876 test rsquared 0.907381394703\n",
      "iteration 15 train rsquared 0.915317458568 test rsquared 0.915077502622\n",
      "iteration 16 train rsquared 0.917341289706 test rsquared 0.917067766186\n",
      "iteration 17 train rsquared 0.920545578241 test rsquared 0.920172191846\n",
      "iteration 18 train rsquared 0.925937436878 test rsquared 0.925486771195\n",
      "iteration 19 train rsquared 0.931261760043 test rsquared 0.930783382596\n"
     ]
    }
   ],
   "source": [
    "X = train[features].values\n",
    "y = train[target].values\n",
    "X_test = valid[features].values\n",
    "y_test = valid[target].values\n",
    "\n",
    "autoencoder_SST_train = np.sum((X - np.mean(X,axis=0))**2)/X.size\n",
    "autoencoder_SST_test = np.sum((X_test - np.mean(X,axis=0))**2)/X_test.size\n",
    "\n",
    "#get model parameters of last run by interation\n",
    "kml.model.get_param_by_iter()\n",
    "kml.model.get_loss_by_iter()\n",
    "\n",
    "print('performance by run')\n",
    "for i in range(parameter_by_run.shape[0]):\n",
    "    w=parameter_by_run[i].copy()\n",
    "    autoencoder_SSE_train = autoencoder_function(X,X,w,shapes+[link])\n",
    "    autoencoder_SSE_test = autoencoder_function(X_test,X_test,w,shapes+[link])\n",
    "    print('iteration',i,'train rsquared',1-autoencoder_SSE_train/autoencoder_SST_train,'test rsquared',1-autoencoder_SSE_test/autoencoder_SST_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1,a1,b2,a2 [  1.10137043e-02   6.65587925e-02   2.70340179e+01   2.28022189e-01]\n",
      "b1,a1,b2,a2 [  1.10137043e-02   6.65587925e-02   2.70340179e+01   2.28022189e-01]\n",
      "b1,a1,b2,a2 [  1.20476708e-02   5.99997320e-02   1.75996897e+01   2.57099810e-01]\n",
      "b1,a1,b2,a2 [  1.20476708e-02   5.99997320e-02   1.75996897e+01   2.57099810e-01]\n",
      "b1,a1,b2,a2 [  1.11272584e-02   5.87126935e-02   1.70730608e+01   2.35982711e-01]\n",
      "b1,a1,b2,a2 [  1.11272584e-02   5.87126935e-02   1.70730608e+01   2.35982711e-01]\n",
      "b1,a1,b2,a2 [ 0.01139576  0.05397906  0.82184919  0.26527898]\n",
      "b1,a1,b2,a2 [ 0.01139576  0.05397906  0.82184919  0.26527898]\n",
      "b1,a1,b2,a2 [  0.01227155   0.05629844  12.04195415   0.24423746]\n",
      "b1,a1,b2,a2 [  0.01227155   0.05629844  12.04195415   0.24423746]\n"
     ]
    }
   ],
   "source": [
    "X = train[features].values\n",
    "y = train[target].values\n",
    "X_test = valid[features].values\n",
    "y_test = valid[target].values\n",
    "\n",
    "autoencoder_SST_train = np.sum((X - np.mean(X,axis=0))**2)/X.size\n",
    "autoencoder_SST_test = np.sum((X_test - np.mean(X,axis=0))**2)/X_test.size\n",
    "\n",
    "def get_latent_encoding(X,w_tensor,link):\n",
    "    w_tensor,extra_w = reshape_vector(w_tensor)\n",
    "    b1,a1,b2,a2 = extra_w[:4]\n",
    "    pred = X.dot(w_tensor[0])\n",
    "    if link == 'linear':\n",
    "        pred = a1*(pred+b1)\n",
    "    elif link == 'field_eq':\n",
    "        pred = -0.5*a1*pred + b1*pred\n",
    "    elif link == 'log':\n",
    "        pred = a1*np.log(np.abs(pred)+b1)\n",
    "    elif link == 'inverse':\n",
    "        pred = a1/(pred+b1)\n",
    "\n",
    "    return pred\n",
    "\n",
    "def reshape_vector(w):\n",
    "    reshape_w = []\n",
    "    indx = 0\n",
    "    for shape,num in zip(shapes[0],shapes[1]):\n",
    "        x = w[indx:num+indx]\n",
    "        if x.size!=num:\n",
    "            continue\n",
    "        x = x.reshape(shape,int(num/shape))\n",
    "        reshape_w.append(x)\n",
    "        indx = indx+num\n",
    "    extra_w = w[indx:]\n",
    "    print('b1,a1,b2,a2',extra_w)\n",
    "    return reshape_w,extra_w\n",
    "\n",
    "\n",
    "#just for fun, we are going to use the latent variables in a predictive model\n",
    "num_encodings = 5\n",
    "encoding_dim = 10\n",
    "X_prime = np.zeros((X.shape[0],num_encodings*encoding_dim))\n",
    "X_test_prime = np.zeros((X_test.shape[0],num_encodings*encoding_dim))\n",
    "\n",
    "#lets sample the last three iterations every 2 step (to avoid similarities)\n",
    "start = 0\n",
    "for i in np.arange(10,20,2):\n",
    "    w=parameter_by_run[i].copy()\n",
    "    X_prime[:,start:start+encoding_dim] = get_latent_encoding(X,w,link)\n",
    "    X_test_prime[:,start:start+encoding_dim] = get_latent_encoding(X_test,w,link)\n",
    "    start = start+encoding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFJCAYAAACCQLQfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11efabb00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lets combine the latent variables with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X_prime)\n",
    "    \n",
    "plt.plot(pca.explained_variance_)\n",
    "plt.show()\n",
    "\n",
    "X_prime = pca.transform(X_prime)\n",
    "X_test_prime = pca.transform(X_test_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add the original variables to the features set\n",
    "X_prime = np.column_stack((X,X_prime))\n",
    "X_test_prime = np.column_stack((X_test,X_test_prime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124999 samples, validate on 125001 samples\n",
      "Epoch 1/100\n",
      "124999/124999 [==============================] - 1s 9us/step - loss: 0.5872 - val_loss: 0.5561\n",
      "Epoch 2/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.5496 - val_loss: 0.5444\n",
      "Epoch 3/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.5391 - val_loss: 0.5386\n",
      "Epoch 4/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.5316 - val_loss: 0.5268\n",
      "Epoch 5/100\n",
      "124999/124999 [==============================] - 1s 5us/step - loss: 0.5224 - val_loss: 0.5212\n",
      "Epoch 6/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.5178 - val_loss: 0.5121\n",
      "Epoch 7/100\n",
      "124999/124999 [==============================] - 1s 5us/step - loss: 0.5108 - val_loss: 0.5078\n",
      "Epoch 8/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.5029 - val_loss: 0.5021\n",
      "Epoch 9/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4960 - val_loss: 0.4953\n",
      "Epoch 10/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4903 - val_loss: 0.4872\n",
      "Epoch 11/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4842 - val_loss: 0.4862\n",
      "Epoch 12/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4821 - val_loss: 0.4806\n",
      "Epoch 13/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4776 - val_loss: 0.4800\n",
      "Epoch 14/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4753 - val_loss: 0.4777\n",
      "Epoch 15/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4711 - val_loss: 0.4727\n",
      "Epoch 16/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4675 - val_loss: 0.4726\n",
      "Epoch 17/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4641 - val_loss: 0.4653\n",
      "Epoch 18/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4614 - val_loss: 0.4668\n",
      "Epoch 19/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4593 - val_loss: 0.4642\n",
      "Epoch 20/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4557 - val_loss: 0.4583\n",
      "Epoch 21/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4527 - val_loss: 0.4581\n",
      "Epoch 22/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4514 - val_loss: 0.4587\n",
      "Epoch 23/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4484 - val_loss: 0.4562\n",
      "Epoch 24/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4463 - val_loss: 0.4566\n",
      "Epoch 25/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4454 - val_loss: 0.4525\n",
      "Epoch 26/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4412 - val_loss: 0.4521\n",
      "Epoch 27/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4400 - val_loss: 0.4484\n",
      "Epoch 28/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4387 - val_loss: 0.4474\n",
      "Epoch 29/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4371 - val_loss: 0.4443\n",
      "Epoch 30/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4358 - val_loss: 0.4409\n",
      "Epoch 31/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4340 - val_loss: 0.4409\n",
      "Epoch 32/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4331 - val_loss: 0.4401\n",
      "Epoch 33/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4310 - val_loss: 0.4392\n",
      "Epoch 34/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4306 - val_loss: 0.4357\n",
      "Epoch 35/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4290 - val_loss: 0.4378\n",
      "Epoch 36/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4274 - val_loss: 0.4310\n",
      "Epoch 37/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4268 - val_loss: 0.4356\n",
      "Epoch 38/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4258 - val_loss: 0.4360\n",
      "Epoch 39/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4246 - val_loss: 0.4343\n",
      "Epoch 40/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4233 - val_loss: 0.4302\n",
      "Epoch 41/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4224 - val_loss: 0.4292\n",
      "Epoch 42/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4213 - val_loss: 0.4266\n",
      "Epoch 43/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4203 - val_loss: 0.4244\n",
      "Epoch 44/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4198 - val_loss: 0.4245\n",
      "Epoch 45/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4189 - val_loss: 0.4234\n",
      "Epoch 46/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4182 - val_loss: 0.4232\n",
      "Epoch 47/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4176 - val_loss: 0.4238\n",
      "Epoch 48/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4188 - val_loss: 0.4247\n",
      "Epoch 49/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4164 - val_loss: 0.4166\n",
      "Epoch 50/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4151 - val_loss: 0.4342\n",
      "Epoch 51/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4154 - val_loss: 0.4307\n",
      "Epoch 52/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4138 - val_loss: 0.4246\n",
      "Epoch 53/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4133 - val_loss: 0.4233\n",
      "Epoch 54/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4123 - val_loss: 0.4219\n",
      "Epoch 55/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4104 - val_loss: 0.4153\n",
      "Epoch 56/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4089 - val_loss: 0.4146\n",
      "Epoch 57/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4084 - val_loss: 0.4130\n",
      "Epoch 58/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4078 - val_loss: 0.4136\n",
      "Epoch 59/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4076 - val_loss: 0.4112\n",
      "Epoch 60/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4062 - val_loss: 0.4130\n",
      "Epoch 61/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4069 - val_loss: 0.4105\n",
      "Epoch 62/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4054 - val_loss: 0.4094\n",
      "Epoch 63/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4050 - val_loss: 0.4087\n",
      "Epoch 64/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4050 - val_loss: 0.4078\n",
      "Epoch 65/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4045 - val_loss: 0.4073\n",
      "Epoch 66/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4036 - val_loss: 0.4076\n",
      "Epoch 67/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4032 - val_loss: 0.4079\n",
      "Epoch 68/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4028 - val_loss: 0.4071\n",
      "Epoch 69/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4027 - val_loss: 0.4074\n",
      "Epoch 70/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4027 - val_loss: 0.4069\n",
      "Epoch 71/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4020 - val_loss: 0.4068\n",
      "Epoch 72/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4015 - val_loss: 0.4058\n",
      "Epoch 73/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4011 - val_loss: 0.4051\n",
      "Epoch 74/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4006 - val_loss: 0.4051\n",
      "Epoch 75/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4006 - val_loss: 0.4057\n",
      "Epoch 76/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4002 - val_loss: 0.4046\n",
      "Epoch 77/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.3999 - val_loss: 0.4053\n",
      "Epoch 78/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.3996 - val_loss: 0.4045\n",
      "Epoch 79/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.3994 - val_loss: 0.4031\n",
      "Epoch 80/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.3991 - val_loss: 0.4029\n",
      "Epoch 81/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.3991 - val_loss: 0.4026\n",
      "Epoch 82/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.3987 - val_loss: 0.4030\n",
      "Epoch 83/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.3984 - val_loss: 0.4032\n",
      "Epoch 84/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.3981 - val_loss: 0.4031\n",
      "Epoch 85/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.3979 - val_loss: 0.4035\n",
      "Epoch 86/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.3976 - val_loss: 0.4025\n",
      "Epoch 87/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.3972 - val_loss: 0.4031\n",
      "Epoch 88/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.3970 - val_loss: 0.4030\n",
      "Epoch 89/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.3967 - val_loss: 0.4031\n",
      "Epoch 90/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.3965 - val_loss: 0.4029\n",
      "Epoch 91/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.3962 - val_loss: 0.4022\n",
      "Epoch 92/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.3958 - val_loss: 0.4027\n",
      "Epoch 93/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.3957 - val_loss: 0.4028\n",
      "Epoch 94/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.3955 - val_loss: 0.4031\n",
      "Epoch 95/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.3953 - val_loss: 0.4029\n",
      "Epoch 96/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.3950 - val_loss: 0.4021\n",
      "Epoch 97/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.3948 - val_loss: 0.4020\n",
      "Epoch 98/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.3946 - val_loss: 0.4018\n",
      "Epoch 99/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.3944 - val_loss: 0.4017\n",
      "Epoch 100/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.3942 - val_loss: 0.4023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11ffef780>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model with latent variable PCA components\n",
    "input_data = Input(shape=(X_prime.shape[1],))\n",
    "layer1 = Dense(200, activation='sigmoid')(input_data)\n",
    "layer2 = Dense(1, activation='sigmoid')(layer1)\n",
    "model = Model(input_data, layer2)\n",
    "\n",
    "model.compile(optimizer='adagrad', \n",
    "                    loss='binary_crossentropy')\n",
    "model.fit(X_prime, y,\n",
    "                epochs=100,\n",
    "                batch_size=5000,\n",
    "                shuffle=False,\n",
    "                validation_data=(X_test_prime, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.81844945240438072, 0.82194257554060435)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_acc(predict,y):\n",
    "    return np.sum((predict>0.5).astype(np.int)==y)/len(y)\n",
    "p_test = model.predict(X_test_prime)\n",
    "p_train = model.predict(X_prime)\n",
    "get_acc(p_test,y_test),get_acc(p_train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124999 samples, validate on 125001 samples\n",
      "Epoch 1/100\n",
      "124999/124999 [==============================] - 1s 8us/step - loss: 0.6011 - val_loss: 0.5701\n",
      "Epoch 2/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.5650 - val_loss: 0.5585\n",
      "Epoch 3/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.5556 - val_loss: 0.5567\n",
      "Epoch 4/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.5505 - val_loss: 0.5445\n",
      "Epoch 5/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.5432 - val_loss: 0.5459\n",
      "Epoch 6/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.5416 - val_loss: 0.5397\n",
      "Epoch 7/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.5373 - val_loss: 0.5370\n",
      "Epoch 8/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.5336 - val_loss: 0.5314\n",
      "Epoch 9/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.5288 - val_loss: 0.5301\n",
      "Epoch 10/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.5263 - val_loss: 0.5264\n",
      "Epoch 11/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.5220 - val_loss: 0.5223\n",
      "Epoch 12/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.5186 - val_loss: 0.5193\n",
      "Epoch 13/100\n",
      "124999/124999 [==============================] - 1s 5us/step - loss: 0.5153 - val_loss: 0.5152\n",
      "Epoch 14/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.5128 - val_loss: 0.5140\n",
      "Epoch 15/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.5099 - val_loss: 0.5095\n",
      "Epoch 16/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.5067 - val_loss: 0.5094\n",
      "Epoch 17/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.5048 - val_loss: 0.5052\n",
      "Epoch 18/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.5018 - val_loss: 0.5101\n",
      "Epoch 19/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.5029 - val_loss: 0.5032\n",
      "Epoch 20/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4981 - val_loss: 0.5032\n",
      "Epoch 21/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4958 - val_loss: 0.5010\n",
      "Epoch 22/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4946 - val_loss: 0.4996\n",
      "Epoch 23/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4928 - val_loss: 0.4999\n",
      "Epoch 24/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4909 - val_loss: 0.4971\n",
      "Epoch 25/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4896 - val_loss: 0.4934\n",
      "Epoch 26/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4870 - val_loss: 0.4916\n",
      "Epoch 27/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4864 - val_loss: 0.4892\n",
      "Epoch 28/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4846 - val_loss: 0.4902\n",
      "Epoch 29/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4835 - val_loss: 0.4892\n",
      "Epoch 30/100\n",
      "124999/124999 [==============================] - 1s 5us/step - loss: 0.4823 - val_loss: 0.4862\n",
      "Epoch 31/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4801 - val_loss: 0.4860\n",
      "Epoch 32/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4791 - val_loss: 0.4845\n",
      "Epoch 33/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4777 - val_loss: 0.4834\n",
      "Epoch 34/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4766 - val_loss: 0.4819\n",
      "Epoch 35/100\n",
      "124999/124999 [==============================] - 1s 5us/step - loss: 0.4755 - val_loss: 0.4807\n",
      "Epoch 36/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4744 - val_loss: 0.4801\n",
      "Epoch 37/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4734 - val_loss: 0.4791\n",
      "Epoch 38/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4725 - val_loss: 0.4784\n",
      "Epoch 39/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4715 - val_loss: 0.4774\n",
      "Epoch 40/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4704 - val_loss: 0.4764\n",
      "Epoch 41/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4696 - val_loss: 0.4754\n",
      "Epoch 42/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4687 - val_loss: 0.4740\n",
      "Epoch 43/100\n",
      "124999/124999 [==============================] - 1s 5us/step - loss: 0.4678 - val_loss: 0.4736\n",
      "Epoch 44/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4670 - val_loss: 0.4726\n",
      "Epoch 45/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4661 - val_loss: 0.4717\n",
      "Epoch 46/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4651 - val_loss: 0.4712\n",
      "Epoch 47/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4642 - val_loss: 0.4697\n",
      "Epoch 48/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4636 - val_loss: 0.4692\n",
      "Epoch 49/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4623 - val_loss: 0.4671\n",
      "Epoch 50/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4618 - val_loss: 0.4678\n",
      "Epoch 51/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4608 - val_loss: 0.4694\n",
      "Epoch 52/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4602 - val_loss: 0.4677\n",
      "Epoch 53/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4595 - val_loss: 0.4665\n",
      "Epoch 54/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4585 - val_loss: 0.4647\n",
      "Epoch 55/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4575 - val_loss: 0.4647\n",
      "Epoch 56/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4572 - val_loss: 0.4642\n",
      "Epoch 57/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4565 - val_loss: 0.4626\n",
      "Epoch 58/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4558 - val_loss: 0.4629\n",
      "Epoch 59/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4552 - val_loss: 0.4617\n",
      "Epoch 60/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4546 - val_loss: 0.4615\n",
      "Epoch 61/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4542 - val_loss: 0.4607\n",
      "Epoch 62/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4535 - val_loss: 0.4601\n",
      "Epoch 63/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4529 - val_loss: 0.4596\n",
      "Epoch 64/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4526 - val_loss: 0.4590\n",
      "Epoch 65/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4523 - val_loss: 0.4588\n",
      "Epoch 66/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4515 - val_loss: 0.4581\n",
      "Epoch 67/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4509 - val_loss: 0.4573\n",
      "Epoch 68/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4504 - val_loss: 0.4570\n",
      "Epoch 69/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4498 - val_loss: 0.4562\n",
      "Epoch 70/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4492 - val_loss: 0.4554\n",
      "Epoch 71/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4487 - val_loss: 0.4549\n",
      "Epoch 72/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4482 - val_loss: 0.4541\n",
      "Epoch 73/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4475 - val_loss: 0.4534\n",
      "Epoch 74/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4470 - val_loss: 0.4528\n",
      "Epoch 75/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4465 - val_loss: 0.4520\n",
      "Epoch 76/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4460 - val_loss: 0.4518\n",
      "Epoch 77/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4455 - val_loss: 0.4512\n",
      "Epoch 78/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4450 - val_loss: 0.4509\n",
      "Epoch 79/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4446 - val_loss: 0.4507\n",
      "Epoch 80/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4442 - val_loss: 0.4502\n",
      "Epoch 81/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4437 - val_loss: 0.4499\n",
      "Epoch 82/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4433 - val_loss: 0.4494\n",
      "Epoch 83/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4429 - val_loss: 0.4489\n",
      "Epoch 84/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4424 - val_loss: 0.4486\n",
      "Epoch 85/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4420 - val_loss: 0.4480\n",
      "Epoch 86/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4416 - val_loss: 0.4475\n",
      "Epoch 87/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4411 - val_loss: 0.4470\n",
      "Epoch 88/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4406 - val_loss: 0.4465\n",
      "Epoch 89/100\n",
      "124999/124999 [==============================] - 1s 4us/step - loss: 0.4402 - val_loss: 0.4460\n",
      "Epoch 90/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4398 - val_loss: 0.4455\n",
      "Epoch 91/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4394 - val_loss: 0.4450\n",
      "Epoch 92/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4390 - val_loss: 0.4445\n",
      "Epoch 93/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4385 - val_loss: 0.4441\n",
      "Epoch 94/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4382 - val_loss: 0.4438\n",
      "Epoch 95/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4377 - val_loss: 0.4434\n",
      "Epoch 96/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4374 - val_loss: 0.4430\n",
      "Epoch 97/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4370 - val_loss: 0.4425\n",
      "Epoch 98/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4366 - val_loss: 0.4421\n",
      "Epoch 99/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4362 - val_loss: 0.4417\n",
      "Epoch 100/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4358 - val_loss: 0.4414\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x125ffa5f8>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model without latent variable PCA components\n",
    "input_data = Input(shape=(X.shape[1],))\n",
    "layer1 = Dense(200, activation='sigmoid')(input_data)\n",
    "layer2 = Dense(1, activation='sigmoid')(layer1)\n",
    "model = Model(input_data, layer2)\n",
    "\n",
    "model.compile(optimizer='adagrad', \n",
    "                    loss='binary_crossentropy')\n",
    "model.fit(X, y,\n",
    "                epochs=100,\n",
    "                batch_size=5000,\n",
    "                shuffle=False,\n",
    "                validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.79688162494700043, 0.80040640325122603)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_acc(predict,y):\n",
    "    return np.sum((predict>0.5).astype(np.int)==y)/len(y)\n",
    "p_test = model.predict(X_test)\n",
    "p_train = model.predict(X)\n",
    "get_acc(p_test,y_test),get_acc(p_train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [datasci]",
   "language": "python",
   "name": "Python [datasci]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
