{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing numpy on engine(s)\n",
      "importing scipy on engine(s)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import seaborn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "import kernelml\n",
    "import re\n",
    "\n",
    "from ipyparallel import Client\n",
    "rc = Client(profile='default')\n",
    "dview = rc[:]\n",
    "\n",
    "dview.block = True\n",
    "\n",
    "with dview.sync_imports():\n",
    "    #for some reason, aliases cannot be use\n",
    "    import numpy\n",
    "    import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full = pd.read_csv('DATA/hb_training.csv')\n",
    "test = pd.read_csv('DATA/hb_testing.csv')\n",
    "\n",
    "def change_label(x):\n",
    "    if x =='s':\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "full['Label'] = full['Label'].apply(change_label)\n",
    "EventId = test['EventId']\n",
    "full.drop(['EventId'],axis=1,inplace=True)\n",
    "test.drop(['EventId'],axis=1,inplace=True)\n",
    "features = list(full.columns[:-2])\n",
    "target = list(full.columns[-1:])\n",
    "\n",
    "#randomly sample and split data\n",
    "all_samples=full.index\n",
    "ones = full[full[target].values==1].index\n",
    "zeros = full[full[target].values==0].index\n",
    "ones_rand_sample = np.random.choice(ones, size=int(len(ones)*0.5),replace=False)\n",
    "zeros_rand_sample = np.random.choice(zeros, size=int(len(zeros)*0.5),replace=False)\n",
    "rand_sample  = np.concatenate((ones_rand_sample,zeros_rand_sample))\n",
    "np.random.shuffle(rand_sample)\n",
    "\n",
    "test_sample = np.setdiff1d(all_samples,rand_sample)\n",
    "valid = full.loc[test_sample,:]\n",
    "train = full.loc[rand_sample,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNShapeHelper():\n",
    "\n",
    "    def __init__(self,layer_shape,num_inputs,num_outputs):\n",
    "        \n",
    "        self.N_inputs = num_inputs\n",
    "        self.N_outputs = num_outputs\n",
    "        self.layer_shape = layer_shape\n",
    "        self.N_layers = len(layer_shape)\n",
    "        self.model_shape = []\n",
    "        self.parameter_shape = []\n",
    "        \n",
    "    def get_N_parameters(self):\n",
    "        \n",
    "        self.model_shape.append(self.N_inputs)\n",
    "        input_n_parameters = self.N_inputs*self.layer_shape[0]\n",
    "        N =  input_n_parameters\n",
    "        self.parameter_shape.append(input_n_parameters)\n",
    "        \n",
    "        for i in range(1,self.N_layers):\n",
    "            layer_n_parameters = self.layer_shape[i-1]*self.layer_shape[i]\n",
    "            self.model_shape.append(self.layer_shape[i])\n",
    "            self.parameter_shape.append(layer_n_parameters)\n",
    "            N += layer_n_parameters\n",
    "            \n",
    "        output_n_parameters = self.N_outputs*self.layer_shape[-1]\n",
    "        N += output_n_parameters\n",
    "        self.model_shape.append(self.N_outputs)\n",
    "        self.parameter_shape.append(output_n_parameters)\n",
    "        self.N_parameters = N\n",
    "        return N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[30, 30, 1], [300, 300, 30]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapehelper = NNShapeHelper([10,len(features)],len(features),1)\n",
    "num_parameters = shapehelper.get_N_parameters()\n",
    "shapes = [shapehelper.model_shape,shapehelper.parameter_shape]\n",
    "shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def autoencoder_function(X,y,w_tensor,args,predict=False):\n",
    "    #can't be passed to parallel engines, so I just assign the aliases manually\n",
    "    #this is an improvement from loading the libraries again\n",
    "    np = numpy\n",
    "    stats = scipy.stats\n",
    "\n",
    "    #arguement\n",
    "    model_shape,parameter_shape,link = args\n",
    "    \n",
    "    # define the loss function between predicted output actual output\n",
    "    def nn_autoencoder_loss(hypothesis,y):\n",
    "        return np.sum((hypothesis-y)**2)/y.size\n",
    "\n",
    "    #reshape parameter vector into list of matrices\n",
    "    def reshape_vector(w):\n",
    "        reshape_w = []\n",
    "        indx = 0\n",
    "        for shape,num in zip(model_shape,parameter_shape):\n",
    "            x = w[indx:num+indx]\n",
    "            if x.size!=num:\n",
    "                continue\n",
    "            x = x.reshape(shape,int(num/shape))\n",
    "            reshape_w.append(x)\n",
    "            indx = indx+num\n",
    "        extra_w = w[indx:]\n",
    "        return reshape_w,extra_w\n",
    "        \n",
    "    #Specifies the way the tensors are combined with the inputs\n",
    "    def combine_tensors(X,w_tensor,link):\n",
    "        w_tensor,extra_w = reshape_vector(w_tensor)\n",
    "        b1,a1,b2,a2 = extra_w[:4]\n",
    "        pred = X.dot(w_tensor[0])\n",
    "        #choose link on encoding layer\n",
    "        if link == 'linear':\n",
    "            pred = a1*(pred+b1)\n",
    "        elif link == 'field_eq':\n",
    "            pred = -0.5*a1*pred + b1*pred\n",
    "        elif link == 'log':\n",
    "            pred = a1*pred + np.log(np.abs(pred)+b1)\n",
    "        elif link == 'inverse':\n",
    "            pred = a1/(pred+b1)\n",
    "\n",
    "        pred = pred.dot(w_tensor[1].T)\n",
    "        pred = a2*(pred+b2)\n",
    "        return pred\n",
    "\n",
    "    #we cannot modify pickled memory so create a copy of the parameter vector\n",
    "    w_tensor_copy = w_tensor.copy()\n",
    "    pred = combine_tensors(X,w_tensor_copy,link)\n",
    "    if predict==True:\n",
    "        return pred\n",
    "    loss = nn_autoencoder_loss(pred,y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('run', 0, 'loss', 118581.61905761655, 'time', 6.945137023925781)\n",
      "('run', 1, 'loss', 79973.343561121976, 'time', 7.270709991455078)\n",
      "('run', 2, 'loss', 61833.753072722604, 'time', 6.726554870605469)\n",
      "('run', 3, 'loss', 45269.55007658706, 'time', 7.354658842086792)\n",
      "('run', 4, 'loss', 37854.083592015981, 'time', 7.254078149795532)\n",
      "('run', 5, 'loss', 31462.321705341379, 'time', 7.0880138874053955)\n",
      "('run', 6, 'loss', 25485.813214866153, 'time', 7.324074983596802)\n",
      "('run', 7, 'loss', 22801.721153488041, 'time', 6.710653781890869)\n",
      "('run', 8, 'loss', 21019.597765571536, 'time', 6.69514012336731)\n",
      "('run', 9, 'loss', 20291.333561435433, 'time', 7.408329010009766)\n",
      "('run', 10, 'loss', 18076.217438580556, 'time', 6.27475905418396)\n",
      "('run', 11, 'loss', 16340.302277419214, 'time', 6.280878782272339)\n",
      "('run', 12, 'loss', 13703.477174452481, 'time', 6.37741494178772)\n",
      "('run', 13, 'loss', 12953.809550682059, 'time', 6.824871063232422)\n",
      "('run', 14, 'loss', 11519.554240940533, 'time', 6.251181125640869)\n",
      "('run', 15, 'loss', 10213.716251772992, 'time', 6.576617002487183)\n",
      "('run', 16, 'loss', 9752.1188885580796, 'time', 6.1553239822387695)\n",
      "('run', 17, 'loss', 9093.3690492319947, 'time', 9.13358211517334)\n",
      "('run', 18, 'loss', 8046.6137836111466, 'time', 7.449779987335205)\n",
      "('run', 19, 'loss', 8538.9869187535642, 'time', 6.1478259563446045)\n"
     ]
    }
   ],
   "source": [
    "X = train[features].values\n",
    "y = train[target].values\n",
    "\n",
    "#prior parameter sampler (default)\n",
    "def prior_sampler_uniform_distribution(weights,num_param):\n",
    "    return np.random.uniform(low=-1,high=1,size=(num_param,1000))\n",
    "\n",
    "#sampler function (default)\n",
    "def sampler_multivariate_normal_distribution(best_param,\n",
    "                                            param_by_iter,\n",
    "                                            error_by_iter,\n",
    "                                            parameter_update_history,\n",
    "                                            random_sample_num=100):\n",
    "    covariance = np.diag(np.var(parameter_update_history[:,:],axis=1))\n",
    "    best = param_by_iter[np.where(error_by_iter==np.min(error_by_iter))[0]]\n",
    "    mean = best.flatten()\n",
    "    try:\n",
    "        return np.random.multivariate_normal(mean, covariance, (random_sample_num)).T\n",
    "    except:\n",
    "        print(best,np.where(error_by_iter==np.min(error_by_iter)))\n",
    "\n",
    "#intermediate sampler\n",
    "def intermediate_uniform_distribution(weights,num_param):\n",
    "    result = []\n",
    "    for i in range(num_param):\n",
    "        x = np.random.uniform(weights[i]-0.1*weights[i],weights[i]+0.1*weights[i],size=(1,10000)).T\n",
    "        result.append(x)\n",
    "    result = np.squeeze(np.array(result))\n",
    "    return result          \n",
    "\n",
    "#parameter transform\n",
    "def positive_int_transform(w):\n",
    "    out = w.copy()\n",
    "    extra_w = out[-4:]\n",
    "    for i in range(extra_w.shape[0]):\n",
    "        extra_w[i][np.where(extra_w[i]<=0)[0]]=1e-6\n",
    "    out[-4:] = extra_w\n",
    "    return out.reshape(w.shape)\n",
    "\n",
    "runs=20\n",
    "zscore = 2.0\n",
    "umagnitude = 0.00001\n",
    "analyzenparam = 100\n",
    "nupdates = 1\n",
    "npriorsamples=3600\n",
    "nrandomsamples = 2400\n",
    "tinterations = 5\n",
    "sequpdate = False\n",
    "link = 'field_eq'\n",
    "\n",
    "kml = kernelml.KernelML(\n",
    "         prior_sampler_fcn=None,\n",
    "         sampler_fcn=None,\n",
    "         intermediate_sampler_fcn=None,\n",
    "         parameter_transform_fcn=positive_int_transform,\n",
    "         batch_size=500)\n",
    "\n",
    "kml.use_ipyparallel(dview)\n",
    "\n",
    "parameter_by_run = kml.optimize(X,X,loss_function=autoencoder_function,\n",
    "                                num_param=num_parameters+4,\n",
    "                                args=shapes+[link],\n",
    "                                runs=runs,\n",
    "                                total_iterations=tinterations,\n",
    "                                analyze_n_parameters=analyzenparam,\n",
    "                                n_parameter_updates=nupdates,\n",
    "                                update_magnitude=umagnitude,\n",
    "                                sequential_update=sequpdate,\n",
    "                                percent_of_params_updated=0.8,\n",
    "                                init_random_sample_num=npriorsamples,\n",
    "                                random_sample_num=nrandomsamples,\n",
    "                                prior_uniform_low=-1,\n",
    "                                prior_uniform_high=1,\n",
    "                                convergence_z_score=1,\n",
    "                                plot_feedback=False,\n",
    "                                print_feedback=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance by run\n",
      "iteration 0 train rsquared -0.400121121979 test rsquared -0.399433360026\n",
      "iteration 1 train rsquared 0.091228360953 test rsquared 0.091880154334\n",
      "iteration 2 train rsquared 0.310196271119 test rsquared 0.307456418755\n",
      "iteration 3 train rsquared 0.491020580736 test rsquared 0.4895940897\n",
      "iteration 4 train rsquared 0.594719767037 test rsquared 0.593908715283\n",
      "iteration 5 train rsquared 0.655287678094 test rsquared 0.653841586177\n",
      "iteration 6 train rsquared 0.706102878768 test rsquared 0.705141554673\n",
      "iteration 7 train rsquared 0.740608194711 test rsquared 0.740224751344\n",
      "iteration 8 train rsquared 0.757315301802 test rsquared 0.757076071973\n",
      "iteration 9 train rsquared 0.776266687732 test rsquared 0.775299223547\n",
      "iteration 10 train rsquared 0.80054966474 test rsquared 0.799886728247\n",
      "iteration 11 train rsquared 0.820589934545 test rsquared 0.819579361335\n",
      "iteration 12 train rsquared 0.843075669249 test rsquared 0.842149941535\n",
      "iteration 13 train rsquared 0.859431265489 test rsquared 0.85847369366\n",
      "iteration 14 train rsquared 0.872060623992 test rsquared 0.871122711432\n",
      "iteration 15 train rsquared 0.88124290513 test rsquared 0.880515224208\n",
      "iteration 16 train rsquared 0.888232263187 test rsquared 0.887547231883\n",
      "iteration 17 train rsquared 0.894763719132 test rsquared 0.89393903794\n",
      "iteration 18 train rsquared 0.903586197496 test rsquared 0.903048809085\n",
      "iteration 19 train rsquared 0.905512438463 test rsquared 0.904925301254\n"
     ]
    }
   ],
   "source": [
    "X = train[features].values\n",
    "y = train[target].values\n",
    "X_test = valid[features].values\n",
    "y_test = valid[target].values\n",
    "\n",
    "autoencoder_SST_train = np.sum((X - np.mean(X,axis=0))**2)/X.size\n",
    "autoencoder_SST_test = np.sum((X_test - np.mean(X,axis=0))**2)/X_test.size\n",
    "\n",
    "#get model parameters of last run by interation\n",
    "kml.model.get_param_by_iter()\n",
    "kml.model.get_loss_by_iter()\n",
    "\n",
    "print('performance by run')\n",
    "for i in range(parameter_by_run.shape[0]):\n",
    "    w=parameter_by_run[i].copy()\n",
    "    autoencoder_SSE_train = autoencoder_function(X,X,w,shapes+[link])\n",
    "    autoencoder_SSE_test = autoencoder_function(X_test,X_test,w,shapes+[link])\n",
    "    print('iteration',i,'train rsquared',1-autoencoder_SSE_train/autoencoder_SST_train,'test rsquared',1-autoencoder_SSE_test/autoencoder_SST_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1,a1,b2,a2 [  0.24068194   0.45490596  50.31629885   0.40663038]\n",
      "b1,a1,b2,a2 [  0.24068194   0.45490596  50.31629885   0.40663038]\n",
      "b1,a1,b2,a2 [  0.20899822   0.39406957  47.20110441   0.3851456 ]\n",
      "b1,a1,b2,a2 [  0.20899822   0.39406957  47.20110441   0.3851456 ]\n",
      "b1,a1,b2,a2 [  0.21892268   0.41899669  47.93216317   0.42063756]\n",
      "b1,a1,b2,a2 [  0.21892268   0.41899669  47.93216317   0.42063756]\n",
      "b1,a1,b2,a2 [ 0.18786113  0.35810506  5.56977594  0.41932687]\n",
      "b1,a1,b2,a2 [ 0.18786113  0.35810506  5.56977594  0.41932687]\n",
      "b1,a1,b2,a2 [ 0.15943045  0.3027852   0.02484501  0.42668913]\n",
      "b1,a1,b2,a2 [ 0.15943045  0.3027852   0.02484501  0.42668913]\n"
     ]
    }
   ],
   "source": [
    "X = train[features].values\n",
    "y = train[target].values\n",
    "X_test = valid[features].values\n",
    "y_test = valid[target].values\n",
    "\n",
    "autoencoder_SST_train = np.sum((X - np.mean(X,axis=0))**2)/X.size\n",
    "autoencoder_SST_test = np.sum((X_test - np.mean(X,axis=0))**2)/X_test.size\n",
    "\n",
    "def get_latent_encoding(X,w_tensor,link):\n",
    "    w_tensor,extra_w = reshape_vector(w_tensor)\n",
    "    b1,a1,b2,a2 = extra_w[:4]\n",
    "    pred = X.dot(w_tensor[0])\n",
    "    if link == 'linear':\n",
    "        pred = a1*(pred+b1)\n",
    "    elif link == 'field_eq':\n",
    "        pred = -0.5*a1*pred + b1*pred\n",
    "    elif link == 'log':\n",
    "        pred = a1*np.log(np.abs(pred)+b1)\n",
    "    elif link == 'inverse':\n",
    "        pred = a1/(pred+b1)\n",
    "\n",
    "    return pred\n",
    "\n",
    "def reshape_vector(w):\n",
    "    reshape_w = []\n",
    "    indx = 0\n",
    "    for shape,num in zip(shapes[0],shapes[1]):\n",
    "        x = w[indx:num+indx]\n",
    "        if x.size!=num:\n",
    "            continue\n",
    "        x = x.reshape(shape,int(num/shape))\n",
    "        reshape_w.append(x)\n",
    "        indx = indx+num\n",
    "    extra_w = w[indx:]\n",
    "    print('b1,a1,b2,a2',extra_w)\n",
    "    return reshape_w,extra_w\n",
    "\n",
    "\n",
    "#just for fun, we are going to use the latent variables in a predictive model\n",
    "num_encodings = 5\n",
    "encoding_dim = 10\n",
    "X_prime = np.zeros((X.shape[0],num_encodings*encoding_dim))\n",
    "X_test_prime = np.zeros((X_test.shape[0],num_encodings*encoding_dim))\n",
    "\n",
    "#lets sample the last three iterations every 2 step (to avoid similarities)\n",
    "start = 0\n",
    "for i in np.arange(10,20,2):\n",
    "    w=parameter_by_run[i].copy()\n",
    "    X_prime[:,start:start+encoding_dim] = get_latent_encoding(X,w,link)\n",
    "    X_test_prime[:,start:start+encoding_dim] = get_latent_encoding(X_test,w,link)\n",
    "    start = start+encoding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAFJCAYAAACCQLQfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11643f278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lets combine the latent variables with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X_prime)\n",
    "    \n",
    "plt.plot(pca.explained_variance_)\n",
    "plt.show()\n",
    "\n",
    "X_prime = pca.transform(X_prime)\n",
    "X_test_prime = pca.transform(X_test_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add the original variables to the features set\n",
    "X_prime = np.column_stack((X,X_prime))\n",
    "X_test_prime = np.column_stack((X_test,X_test_prime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "def get_acc(predict,y):\n",
    "    return np.sum((predict>0.5).astype(np.int)==y)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124999 samples, validate on 125001 samples\n",
      "Epoch 1/100\n",
      "124999/124999 [==============================] - 1s 5us/step - loss: 0.5913 - val_loss: 0.5682\n",
      "Epoch 2/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5625 - val_loss: 0.5563\n",
      "Epoch 3/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.5511 - val_loss: 0.5467\n",
      "Epoch 4/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5422 - val_loss: 0.5384\n",
      "Epoch 5/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5351 - val_loss: 0.5321\n",
      "Epoch 6/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5293 - val_loss: 0.5256\n",
      "Epoch 7/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5250 - val_loss: 0.5209\n",
      "Epoch 8/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5195 - val_loss: 0.5174\n",
      "Epoch 9/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5145 - val_loss: 0.5111\n",
      "Epoch 10/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5095 - val_loss: 0.5060\n",
      "Epoch 11/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5057 - val_loss: 0.5019\n",
      "Epoch 12/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5011 - val_loss: 0.4976\n",
      "Epoch 13/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4971 - val_loss: 0.4932\n",
      "Epoch 14/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4942 - val_loss: 0.4899\n",
      "Epoch 15/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4908 - val_loss: 0.4880\n",
      "Epoch 16/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4880 - val_loss: 0.4847\n",
      "Epoch 17/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4859 - val_loss: 0.4832\n",
      "Epoch 18/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4821 - val_loss: 0.4781\n",
      "Epoch 19/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4775 - val_loss: 0.4736\n",
      "Epoch 20/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4739 - val_loss: 0.4761\n",
      "Epoch 21/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4735 - val_loss: 0.4705\n",
      "Epoch 22/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4695 - val_loss: 0.4670\n",
      "Epoch 23/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4674 - val_loss: 0.4648\n",
      "Epoch 24/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4653 - val_loss: 0.4632\n",
      "Epoch 25/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4634 - val_loss: 0.4605\n",
      "Epoch 26/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4610 - val_loss: 0.4592\n",
      "Epoch 27/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4595 - val_loss: 0.4583\n",
      "Epoch 28/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4584 - val_loss: 0.4575\n",
      "Epoch 29/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4580 - val_loss: 0.4558\n",
      "Epoch 30/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4558 - val_loss: 0.4540\n",
      "Epoch 31/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4537 - val_loss: 0.4527\n",
      "Epoch 32/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4523 - val_loss: 0.4514\n",
      "Epoch 33/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4506 - val_loss: 0.4496\n",
      "Epoch 34/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4496 - val_loss: 0.4493\n",
      "Epoch 35/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4481 - val_loss: 0.4464\n",
      "Epoch 36/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4470 - val_loss: 0.4467\n",
      "Epoch 37/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4458 - val_loss: 0.4443\n",
      "Epoch 38/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4444 - val_loss: 0.4434\n",
      "Epoch 39/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4433 - val_loss: 0.4421\n",
      "Epoch 40/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4423 - val_loss: 0.4402\n",
      "Epoch 41/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4410 - val_loss: 0.4395\n",
      "Epoch 42/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4401 - val_loss: 0.4384\n",
      "Epoch 43/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4391 - val_loss: 0.4382\n",
      "Epoch 44/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4385 - val_loss: 0.4371\n",
      "Epoch 45/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4376 - val_loss: 0.4360\n",
      "Epoch 46/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4368 - val_loss: 0.4357\n",
      "Epoch 47/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4359 - val_loss: 0.4348\n",
      "Epoch 48/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4350 - val_loss: 0.4345\n",
      "Epoch 49/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4342 - val_loss: 0.4336\n",
      "Epoch 50/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4333 - val_loss: 0.4327\n",
      "Epoch 51/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4330 - val_loss: 0.4324\n",
      "Epoch 52/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4319 - val_loss: 0.4308\n",
      "Epoch 53/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4312 - val_loss: 0.4300\n",
      "Epoch 54/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4312 - val_loss: 0.4297\n",
      "Epoch 55/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4295 - val_loss: 0.4293\n",
      "Epoch 56/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4293 - val_loss: 0.4285\n",
      "Epoch 57/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4284 - val_loss: 0.4279\n",
      "Epoch 58/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4278 - val_loss: 0.4276\n",
      "Epoch 59/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4272 - val_loss: 0.4268\n",
      "Epoch 60/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4267 - val_loss: 0.4267\n",
      "Epoch 61/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4261 - val_loss: 0.4257\n",
      "Epoch 62/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4253 - val_loss: 0.4250\n",
      "Epoch 63/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4243 - val_loss: 0.4247\n",
      "Epoch 64/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4239 - val_loss: 0.4250\n",
      "Epoch 65/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4236 - val_loss: 0.4241\n",
      "Epoch 66/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4233 - val_loss: 0.4239\n",
      "Epoch 67/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4223 - val_loss: 0.4229\n",
      "Epoch 68/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4217 - val_loss: 0.4224\n",
      "Epoch 69/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4211 - val_loss: 0.4219\n",
      "Epoch 70/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4207 - val_loss: 0.4212\n",
      "Epoch 71/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4202 - val_loss: 0.4210\n",
      "Epoch 72/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4200 - val_loss: 0.4206\n",
      "Epoch 73/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4194 - val_loss: 0.4205\n",
      "Epoch 74/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4189 - val_loss: 0.4198\n",
      "Epoch 75/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4186 - val_loss: 0.4192\n",
      "Epoch 76/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4183 - val_loss: 0.4189\n",
      "Epoch 77/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4177 - val_loss: 0.4186\n",
      "Epoch 78/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4174 - val_loss: 0.4182\n",
      "Epoch 79/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4171 - val_loss: 0.4178\n",
      "Epoch 80/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4166 - val_loss: 0.4175\n",
      "Epoch 81/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4163 - val_loss: 0.4161\n",
      "Epoch 82/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4160 - val_loss: 0.4162\n",
      "Epoch 83/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4156 - val_loss: 0.4157\n",
      "Epoch 84/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4151 - val_loss: 0.4155\n",
      "Epoch 85/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4149 - val_loss: 0.4161\n",
      "Epoch 86/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4147 - val_loss: 0.4151\n",
      "Epoch 87/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4142 - val_loss: 0.4147\n",
      "Epoch 88/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4138 - val_loss: 0.4143\n",
      "Epoch 89/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4134 - val_loss: 0.4138\n",
      "Epoch 90/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4131 - val_loss: 0.4135\n",
      "Epoch 91/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4127 - val_loss: 0.4130\n",
      "Epoch 92/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4123 - val_loss: 0.4127\n",
      "Epoch 93/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4120 - val_loss: 0.4125\n",
      "Epoch 94/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4116 - val_loss: 0.4124\n",
      "Epoch 95/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4114 - val_loss: 0.4129\n",
      "Epoch 96/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4111 - val_loss: 0.4129\n",
      "Epoch 97/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4109 - val_loss: 0.4118\n",
      "Epoch 98/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4106 - val_loss: 0.4112\n",
      "Epoch 99/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4103 - val_loss: 0.4113\n",
      "Epoch 100/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4100 - val_loss: 0.4112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1181cb8d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model with latent variable PCA components\n",
    "input_data = Input(shape=(X_prime.shape[1],))\n",
    "layer1 = Dense(200, activation='sigmoid')(input_data)\n",
    "layer2 = Dense(1, activation='sigmoid')(layer1)\n",
    "model = Model(input_data, layer2)\n",
    "\n",
    "model.compile(optimizer='adagrad', \n",
    "                    loss='binary_crossentropy')\n",
    "model.fit(X_prime, y,\n",
    "                epochs=100,\n",
    "                batch_size=5000,\n",
    "                shuffle=False,\n",
    "                validation_data=(X_test_prime, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8203694370445036, 0.82114256914055317)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test = model.predict(X_test_prime)\n",
    "p_train = model.predict(X_prime)\n",
    "get_acc(p_test,y_test),get_acc(p_train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124999 samples, validate on 125001 samples\n",
      "Epoch 1/100\n",
      "124999/124999 [==============================] - 1s 5us/step - loss: 0.5870 - val_loss: 0.5686\n",
      "Epoch 2/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5583 - val_loss: 0.5533\n",
      "Epoch 3/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5523 - val_loss: 0.5524\n",
      "Epoch 4/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5463 - val_loss: 0.5416\n",
      "Epoch 5/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5410 - val_loss: 0.5379\n",
      "Epoch 6/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5364 - val_loss: 0.5348\n",
      "Epoch 7/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5336 - val_loss: 0.5297\n",
      "Epoch 8/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5301 - val_loss: 0.5256\n",
      "Epoch 9/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5265 - val_loss: 0.5253\n",
      "Epoch 10/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5239 - val_loss: 0.5220\n",
      "Epoch 11/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.5207 - val_loss: 0.5190\n",
      "Epoch 12/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5190 - val_loss: 0.5169\n",
      "Epoch 13/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5162 - val_loss: 0.5145\n",
      "Epoch 14/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5142 - val_loss: 0.5132\n",
      "Epoch 15/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5128 - val_loss: 0.5117\n",
      "Epoch 16/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.5104 - val_loss: 0.5106\n",
      "Epoch 17/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5090 - val_loss: 0.5084\n",
      "Epoch 18/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5078 - val_loss: 0.5057\n",
      "Epoch 19/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5050 - val_loss: 0.5035\n",
      "Epoch 20/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5031 - val_loss: 0.5025\n",
      "Epoch 21/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5018 - val_loss: 0.5013\n",
      "Epoch 22/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5000 - val_loss: 0.5013\n",
      "Epoch 23/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4998 - val_loss: 0.4983\n",
      "Epoch 24/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4974 - val_loss: 0.4977\n",
      "Epoch 25/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4970 - val_loss: 0.4952\n",
      "Epoch 26/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4944 - val_loss: 0.4943\n",
      "Epoch 27/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4937 - val_loss: 0.4932\n",
      "Epoch 28/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4931 - val_loss: 0.4921\n",
      "Epoch 29/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4909 - val_loss: 0.4912\n",
      "Epoch 30/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4896 - val_loss: 0.4900\n",
      "Epoch 31/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4895 - val_loss: 0.4892\n",
      "Epoch 32/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4878 - val_loss: 0.4882\n",
      "Epoch 33/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4867 - val_loss: 0.4867\n",
      "Epoch 34/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4857 - val_loss: 0.4865\n",
      "Epoch 35/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4847 - val_loss: 0.4854\n",
      "Epoch 36/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4836 - val_loss: 0.4841\n",
      "Epoch 37/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4826 - val_loss: 0.4833\n",
      "Epoch 38/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4819 - val_loss: 0.4823\n",
      "Epoch 39/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4813 - val_loss: 0.4820\n",
      "Epoch 40/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4803 - val_loss: 0.4808\n",
      "Epoch 41/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4792 - val_loss: 0.4797\n",
      "Epoch 42/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4783 - val_loss: 0.4788\n",
      "Epoch 43/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4775 - val_loss: 0.4778\n",
      "Epoch 44/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4778 - val_loss: 0.4778\n",
      "Epoch 45/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4760 - val_loss: 0.4763\n",
      "Epoch 46/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4762 - val_loss: 0.4757\n",
      "Epoch 47/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4743 - val_loss: 0.4754\n",
      "Epoch 48/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4739 - val_loss: 0.4736\n",
      "Epoch 49/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4728 - val_loss: 0.4736\n",
      "Epoch 50/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4724 - val_loss: 0.4721\n",
      "Epoch 51/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4723 - val_loss: 0.4721\n",
      "Epoch 52/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4711 - val_loss: 0.4711\n",
      "Epoch 53/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4703 - val_loss: 0.4704\n",
      "Epoch 54/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4696 - val_loss: 0.4707\n",
      "Epoch 55/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4689 - val_loss: 0.4691\n",
      "Epoch 56/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4686 - val_loss: 0.4699\n",
      "Epoch 57/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4672 - val_loss: 0.4680\n",
      "Epoch 58/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4666 - val_loss: 0.4683\n",
      "Epoch 59/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4661 - val_loss: 0.4670\n",
      "Epoch 60/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4654 - val_loss: 0.4676\n",
      "Epoch 61/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4650 - val_loss: 0.4667\n",
      "Epoch 62/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4642 - val_loss: 0.4665\n",
      "Epoch 63/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4637 - val_loss: 0.4656\n",
      "Epoch 64/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4629 - val_loss: 0.4651\n",
      "Epoch 65/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4624 - val_loss: 0.4635\n",
      "Epoch 66/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4615 - val_loss: 0.4630\n",
      "Epoch 67/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4606 - val_loss: 0.4621\n",
      "Epoch 68/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4598 - val_loss: 0.4616\n",
      "Epoch 69/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4594 - val_loss: 0.4609\n",
      "Epoch 70/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4588 - val_loss: 0.4605\n",
      "Epoch 71/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4581 - val_loss: 0.4599\n",
      "Epoch 72/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4576 - val_loss: 0.4592\n",
      "Epoch 73/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4571 - val_loss: 0.4585\n",
      "Epoch 74/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4564 - val_loss: 0.4579\n",
      "Epoch 75/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4558 - val_loss: 0.4575\n",
      "Epoch 76/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4553 - val_loss: 0.4570\n",
      "Epoch 77/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4547 - val_loss: 0.4565\n",
      "Epoch 78/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4547 - val_loss: 0.4559\n",
      "Epoch 79/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4539 - val_loss: 0.4559\n",
      "Epoch 80/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4537 - val_loss: 0.4548\n",
      "Epoch 81/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4528 - val_loss: 0.4546\n",
      "Epoch 82/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4524 - val_loss: 0.4539\n",
      "Epoch 83/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4520 - val_loss: 0.4537\n",
      "Epoch 84/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4516 - val_loss: 0.4533\n",
      "Epoch 85/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4512 - val_loss: 0.4530\n",
      "Epoch 86/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4508 - val_loss: 0.4526\n",
      "Epoch 87/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4504 - val_loss: 0.4523\n",
      "Epoch 88/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4499 - val_loss: 0.4518\n",
      "Epoch 89/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4496 - val_loss: 0.4513\n",
      "Epoch 90/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4492 - val_loss: 0.4509\n",
      "Epoch 91/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4488 - val_loss: 0.4505\n",
      "Epoch 92/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4483 - val_loss: 0.4501\n",
      "Epoch 93/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4479 - val_loss: 0.4498\n",
      "Epoch 94/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4476 - val_loss: 0.4494\n",
      "Epoch 95/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4471 - val_loss: 0.4490\n",
      "Epoch 96/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.4471 - val_loss: 0.4477\n",
      "Epoch 97/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4459 - val_loss: 0.4447\n",
      "Epoch 98/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4426 - val_loss: 0.4448\n",
      "Epoch 99/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4423 - val_loss: 0.4438\n",
      "Epoch 100/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4416 - val_loss: 0.4433\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x119228f60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model without latent variable PCA components\n",
    "input_data = Input(shape=(X.shape[1],))\n",
    "layer1 = Dense(200, activation='sigmoid')(input_data)\n",
    "layer2 = Dense(1, activation='sigmoid')(layer1)\n",
    "model = Model(input_data, layer2)\n",
    "\n",
    "model.compile(optimizer='adagrad', \n",
    "                    loss='binary_crossentropy')\n",
    "model.fit(X, y,\n",
    "                epochs=100,\n",
    "                batch_size=5000,\n",
    "                shuffle=False,\n",
    "                validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8008655930752554, 0.8031184249473996)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test = model.predict(X_test)\n",
    "p_train = model.predict(X)\n",
    "get_acc(p_test,y_test),get_acc(p_train,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [datasci]",
   "language": "python",
   "name": "Python [datasci]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
