{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing numpy on engine(s)\n",
      "importing scipy on engine(s)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import seaborn\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn import linear_model\n",
    "import kernelml\n",
    "import re\n",
    "\n",
    "from ipyparallel import Client\n",
    "rc = Client(profile='default')\n",
    "dview = rc[:]\n",
    "\n",
    "dview.block = True\n",
    "\n",
    "with dview.sync_imports():\n",
    "    #for some reason, aliases cannot be use\n",
    "    import numpy\n",
    "    import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "full = pd.read_csv('DATA/hb_training.csv')\n",
    "test = pd.read_csv('DATA/hb_testing.csv')\n",
    "\n",
    "def change_label(x):\n",
    "    if x =='s':\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "    \n",
    "full['Label'] = full['Label'].apply(change_label)\n",
    "EventId = test['EventId']\n",
    "full.drop(['EventId'],axis=1,inplace=True)\n",
    "test.drop(['EventId'],axis=1,inplace=True)\n",
    "features = list(full.columns[:-2])\n",
    "target = list(full.columns[-1:])\n",
    "\n",
    "#randomly sample and split data\n",
    "all_samples=full.index\n",
    "ones = full[full[target].values==1].index\n",
    "zeros = full[full[target].values==0].index\n",
    "ones_rand_sample = np.random.choice(ones, size=int(len(ones)*0.5),replace=False)\n",
    "zeros_rand_sample = np.random.choice(zeros, size=int(len(zeros)*0.5),replace=False)\n",
    "rand_sample  = np.concatenate((ones_rand_sample,zeros_rand_sample))\n",
    "np.random.shuffle(rand_sample)\n",
    "\n",
    "test_sample = np.setdiff1d(all_samples,rand_sample)\n",
    "valid = full.loc[test_sample,:]\n",
    "train = full.loc[rand_sample,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NNShapeHelper():\n",
    "\n",
    "    def __init__(self,layer_shape,num_inputs,num_outputs):\n",
    "        \n",
    "        self.N_inputs = num_inputs\n",
    "        self.N_outputs = num_outputs\n",
    "        self.layer_shape = layer_shape\n",
    "        self.N_layers = len(layer_shape)\n",
    "        self.model_shape = []\n",
    "        self.parameter_shape = []\n",
    "        \n",
    "    def get_N_parameters(self):\n",
    "        \n",
    "        self.model_shape.append(self.N_inputs)\n",
    "        input_n_parameters = self.N_inputs*self.layer_shape[0]\n",
    "        N =  input_n_parameters\n",
    "        self.parameter_shape.append(input_n_parameters)\n",
    "        \n",
    "        for i in range(1,self.N_layers):\n",
    "            layer_n_parameters = self.layer_shape[i-1]*self.layer_shape[i]\n",
    "            self.model_shape.append(self.layer_shape[i])\n",
    "            self.parameter_shape.append(layer_n_parameters)\n",
    "            N += layer_n_parameters\n",
    "            \n",
    "        output_n_parameters = self.N_outputs*self.layer_shape[-1]\n",
    "        N += output_n_parameters\n",
    "        self.model_shape.append(self.N_outputs)\n",
    "        self.parameter_shape.append(output_n_parameters)\n",
    "        self.N_parameters = N\n",
    "        return N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[30, 30, 1], [300, 300, 30]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapehelper = NNShapeHelper([10,len(features)],len(features),1)\n",
    "num_parameters = shapehelper.get_N_parameters()\n",
    "shapes = [shapehelper.model_shape,shapehelper.parameter_shape]\n",
    "shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def autoencoder_function(X,y,w_tensor,args,predict=False):\n",
    "    #can't be passed to parallel engines, so I just assign the aliases manually\n",
    "    #this is an improvement from loading the libraries again\n",
    "    np = numpy\n",
    "    stats = scipy.stats\n",
    "\n",
    "    #arguement\n",
    "    model_shape,parameter_shape,link = args\n",
    "    \n",
    "    # define the loss function between predicted output actual output\n",
    "    def nn_autoencoder_loss(hypothesis,y):\n",
    "        return np.sum((hypothesis-y)**2)/y.size\n",
    "\n",
    "    #reshape parameter vector into list of matrices\n",
    "    def reshape_vector(w):\n",
    "        reshape_w = []\n",
    "        indx = 0\n",
    "        for shape,num in zip(model_shape,parameter_shape):\n",
    "            x = w[indx:num+indx]\n",
    "            if x.size!=num:\n",
    "                continue\n",
    "            x = x.reshape(shape,int(num/shape))\n",
    "            reshape_w.append(x)\n",
    "            indx = indx+num\n",
    "        extra_w = w[indx:]\n",
    "        return reshape_w,extra_w\n",
    "        \n",
    "    #Specifies the way the tensors are combined with the inputs\n",
    "    def combine_tensors(X,w_tensor,link):\n",
    "        w_tensor,extra_w = reshape_vector(w_tensor)\n",
    "        b1,a1,b2,a2 = extra_w[:4]\n",
    "        pred = X.dot(w_tensor[0])\n",
    "        #choose link on encoding layer\n",
    "        if link == 'linear':\n",
    "            pred = a1*(pred+b1)\n",
    "        elif link == 'field_eq':\n",
    "            pred = -0.5*a1*pred + b1*pred\n",
    "        elif link == 'log':\n",
    "            pred = a1*pred + np.log(np.abs(pred)+b1)\n",
    "        elif link == 'inverse':\n",
    "            pred = a1/(pred+b1)\n",
    "\n",
    "        pred = pred.dot(w_tensor[1].T)\n",
    "        pred = a2*(pred+b2)\n",
    "        return pred\n",
    "\n",
    "    #we cannot modify pickled memory so create a copy of the parameter vector\n",
    "    w_tensor_copy = w_tensor.copy()\n",
    "    pred = combine_tensors(X,w_tensor_copy,link)\n",
    "    if predict==True:\n",
    "        return pred\n",
    "    loss = nn_autoencoder_loss(pred,y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('run', 0, 'loss', 146578.7846361628, 'time', 6.362144947052002)\n",
      "('run', 1, 'loss', 100267.14193227427, 'time', 7.634387969970703)\n",
      "('run', 2, 'loss', 71972.952330770786, 'time', 8.16092300415039)\n",
      "('run', 3, 'loss', 46520.481271872261, 'time', 7.096730947494507)\n",
      "('run', 4, 'loss', 38567.515028045382, 'time', 6.658151865005493)\n",
      "('run', 5, 'loss', 28465.667047696224, 'time', 6.325037956237793)\n",
      "('run', 6, 'loss', 24851.040072695818, 'time', 6.671178817749023)\n",
      "('run', 7, 'loss', 22961.177597452705, 'time', 6.472333192825317)\n",
      "('run', 8, 'loss', 21961.548513005837, 'time', 6.156144857406616)\n",
      "('run', 9, 'loss', 19469.94914574673, 'time', 5.742694139480591)\n",
      "('run', 10, 'loss', 17366.664179343854, 'time', 6.129958868026733)\n",
      "('run', 11, 'loss', 16774.069730162562, 'time', 5.93103289604187)\n",
      "('run', 12, 'loss', 15290.275569158403, 'time', 5.985960960388184)\n",
      "('run', 13, 'loss', 13245.139523151509, 'time', 5.983272075653076)\n",
      "('run', 14, 'loss', 11214.182089741047, 'time', 6.55915093421936)\n",
      "('run', 15, 'loss', 10779.0252514561, 'time', 5.985840082168579)\n",
      "('run', 16, 'loss', 10016.497001815804, 'time', 6.21665620803833)\n",
      "('run', 17, 'loss', 8738.8995942240508, 'time', 7.064651012420654)\n",
      "('run', 18, 'loss', 8694.1057682827068, 'time', 7.767192840576172)\n",
      "('run', 19, 'loss', 7935.5360821899931, 'time', 7.753856897354126)\n"
     ]
    }
   ],
   "source": [
    "X = train[features].values\n",
    "y = train[target].values\n",
    "\n",
    "#customizable functions\n",
    "\"\"\"\n",
    "#prior parameter sampler (default)\n",
    "def prior_sampler_uniform_distribution(weights,num_param):\n",
    "    return np.random.uniform(low=-1,high=1,size=(num_param,1000))\n",
    "\n",
    "#sampler function (default)\n",
    "def sampler_multivariate_normal_distribution(best_param,\n",
    "                                            param_by_iter,\n",
    "                                            error_by_iter,\n",
    "                                            parameter_update_history,\n",
    "                                            random_sample_num=100):\n",
    "    covariance = np.diag(np.var(parameter_update_history[:,:],axis=1))\n",
    "    best = param_by_iter[np.where(error_by_iter==np.min(error_by_iter))[0]]\n",
    "    mean = best.flatten()\n",
    "    try:\n",
    "        return np.random.multivariate_normal(mean, covariance, (random_sample_num)).T\n",
    "    except:\n",
    "        print(best,np.where(error_by_iter==np.min(error_by_iter)))\n",
    "\n",
    "#intermediate sampler\n",
    "def intermediate_uniform_distribution(weights,num_param):\n",
    "    result = []\n",
    "    for i in range(num_param):\n",
    "        x = np.random.uniform(weights[i]-0.1*weights[i],weights[i]+0.1*weights[i],size=(1,10000)).T\n",
    "        result.append(x)\n",
    "    result = np.squeeze(np.array(result))\n",
    "    return result          \n",
    "\n",
    "#mini batch random choice sampler\n",
    "def mini_batch_random_choice(X,y,batch_size):\n",
    "    all_samples = np.arange(0,X.shape[0])\n",
    "    rand_sample = np.random.choice(all_samples,size=batch_size,replace=False)\n",
    "    X_batch = X[rand_sample]\n",
    "    y_batch = y[rand_sample]\n",
    "    return X_batch,y_batch\n",
    "\"\"\"\n",
    "\n",
    "#parameter transform\n",
    "def positive_int_transform(w):\n",
    "    out = w.copy()\n",
    "    extra_w = out[-4:]\n",
    "    for i in range(extra_w.shape[0]):\n",
    "        extra_w[i][np.where(extra_w[i]<=0)[0]]=1e-6\n",
    "    out[-4:] = extra_w\n",
    "    return out.reshape(w.shape)\n",
    "\n",
    "runs=20\n",
    "zscore = 2.0\n",
    "umagnitude = 0.00001\n",
    "analyzenparam = 100\n",
    "nupdates = 1\n",
    "npriorsamples=3600\n",
    "nrandomsamples = 2400\n",
    "tinterations = 5\n",
    "sequpdate = False\n",
    "link = 'field_eq'\n",
    "\n",
    "kml = kernelml.KernelML(\n",
    "         prior_sampler_fcn=None,\n",
    "         sampler_fcn=None,\n",
    "         intermediate_sampler_fcn=None,\n",
    "         mini_batch_sampler_fcn=None,\n",
    "         parameter_transform_fcn=positive_int_transform,\n",
    "         batch_size=500)\n",
    "\n",
    "kml.use_ipyparallel(dview)\n",
    "\n",
    "parameter_by_run = kml.optimize(X,X,loss_function=autoencoder_function,\n",
    "                                num_param=num_parameters+4,\n",
    "                                args=shapes+[link],\n",
    "                                runs=runs,\n",
    "                                total_iterations=tinterations,\n",
    "                                analyze_n_parameters=analyzenparam,\n",
    "                                n_parameter_updates=nupdates,\n",
    "                                update_magnitude=umagnitude,\n",
    "                                sequential_update=sequpdate,\n",
    "                                percent_of_params_updated=0.8,\n",
    "                                init_random_sample_num=npriorsamples,\n",
    "                                random_sample_num=nrandomsamples,\n",
    "                                prior_uniform_low=-1,\n",
    "                                prior_uniform_high=1,\n",
    "                                convergence_z_score=1,\n",
    "                                plot_feedback=False,\n",
    "                                print_feedback=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performance by run\n",
      "iteration 0 train rsquared -0.686450151062 test rsquared -0.687413466372\n",
      "iteration 1 train rsquared -0.0987589664371 test rsquared -0.0975857943331\n",
      "iteration 2 train rsquared 0.183253058222 test rsquared 0.188085793913\n",
      "iteration 3 train rsquared 0.438715811101 test rsquared 0.44779741171\n",
      "iteration 4 train rsquared 0.584866455489 test rsquared 0.587569810431\n",
      "iteration 5 train rsquared 0.664658885385 test rsquared 0.665465830766\n",
      "iteration 6 train rsquared 0.706943950828 test rsquared 0.708276719209\n",
      "iteration 7 train rsquared 0.739134415297 test rsquared 0.739966841469\n",
      "iteration 8 train rsquared 0.768029913131 test rsquared 0.768805215848\n",
      "iteration 9 train rsquared 0.783132406225 test rsquared 0.783458553416\n",
      "iteration 10 train rsquared 0.806548610964 test rsquared 0.806981511354\n",
      "iteration 11 train rsquared 0.823871430485 test rsquared 0.824510841191\n",
      "iteration 12 train rsquared 0.840289773893 test rsquared 0.840711257835\n",
      "iteration 13 train rsquared 0.851690346057 test rsquared 0.852093079448\n",
      "iteration 14 train rsquared 0.864631957077 test rsquared 0.864971764067\n",
      "iteration 15 train rsquared 0.876058540377 test rsquared 0.8761962123\n",
      "iteration 16 train rsquared 0.890901053886 test rsquared 0.891025010548\n",
      "iteration 17 train rsquared 0.896447092675 test rsquared 0.896524831566\n",
      "iteration 18 train rsquared 0.903825394597 test rsquared 0.903924828118\n",
      "iteration 19 train rsquared 0.905738703232 test rsquared 0.905872907334\n"
     ]
    }
   ],
   "source": [
    "X = train[features].values\n",
    "y = train[target].values\n",
    "X_test = valid[features].values\n",
    "y_test = valid[target].values\n",
    "\n",
    "autoencoder_SST_train = np.sum((X - np.mean(X,axis=0))**2)/X.size\n",
    "autoencoder_SST_test = np.sum((X_test - np.mean(X,axis=0))**2)/X_test.size\n",
    "\n",
    "#get model parameters of last run by interation\n",
    "kml.model.get_param_by_iter()\n",
    "kml.model.get_loss_by_iter()\n",
    "\n",
    "print('performance by run')\n",
    "for i in range(parameter_by_run.shape[0]):\n",
    "    w=parameter_by_run[i].copy()\n",
    "    autoencoder_SSE_train = autoencoder_function(X,X,w,shapes+[link])\n",
    "    autoencoder_SSE_test = autoencoder_function(X_test,X_test,w,shapes+[link])\n",
    "    print('iteration',i,'train rsquared',1-autoencoder_SSE_train/autoencoder_SST_train,'test rsquared',1-autoencoder_SSE_test/autoencoder_SST_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b1,a1,b2,a2 [ 0.03640615  0.11180976  0.03765359  0.17820258]\n",
      "b1,a1,b2,a2 [ 0.03640615  0.11180976  0.03765359  0.17820258]\n",
      "b1,a1,b2,a2 [ 0.0365186   0.10122866  0.3666661   0.20993291]\n",
      "b1,a1,b2,a2 [ 0.0365186   0.10122866  0.3666661   0.20993291]\n",
      "b1,a1,b2,a2 [  3.94071626e-02   1.04246803e-01   1.00000000e-06   2.07515293e-01]\n",
      "b1,a1,b2,a2 [  3.94071626e-02   1.04246803e-01   1.00000000e-06   2.07515293e-01]\n",
      "b1,a1,b2,a2 [ 0.0437813   0.11768894  0.04657603  0.16807767]\n",
      "b1,a1,b2,a2 [ 0.0437813   0.11768894  0.04657603  0.16807767]\n",
      "b1,a1,b2,a2 [ 0.04199653  0.11118947  2.53014692  0.17151564]\n",
      "b1,a1,b2,a2 [ 0.04199653  0.11118947  2.53014692  0.17151564]\n"
     ]
    }
   ],
   "source": [
    "X = train[features].values\n",
    "y = train[target].values\n",
    "X_test = valid[features].values\n",
    "y_test = valid[target].values\n",
    "\n",
    "autoencoder_SST_train = np.sum((X - np.mean(X,axis=0))**2)/X.size\n",
    "autoencoder_SST_test = np.sum((X_test - np.mean(X,axis=0))**2)/X_test.size\n",
    "\n",
    "def get_latent_encoding(X,w_tensor,link):\n",
    "    w_tensor,extra_w = reshape_vector(w_tensor)\n",
    "    b1,a1,b2,a2 = extra_w[:4]\n",
    "    pred = X.dot(w_tensor[0])\n",
    "    if link == 'linear':\n",
    "        pred = a1*(pred+b1)\n",
    "    elif link == 'field_eq':\n",
    "        pred = -0.5*a1*pred + b1*pred\n",
    "    elif link == 'log':\n",
    "        pred = a1*np.log(np.abs(pred)+b1)\n",
    "    elif link == 'inverse':\n",
    "        pred = a1/(pred+b1)\n",
    "\n",
    "    return pred\n",
    "\n",
    "def reshape_vector(w):\n",
    "    reshape_w = []\n",
    "    indx = 0\n",
    "    for shape,num in zip(shapes[0],shapes[1]):\n",
    "        x = w[indx:num+indx]\n",
    "        if x.size!=num:\n",
    "            continue\n",
    "        x = x.reshape(shape,int(num/shape))\n",
    "        reshape_w.append(x)\n",
    "        indx = indx+num\n",
    "    extra_w = w[indx:]\n",
    "    print('b1,a1,b2,a2',extra_w)\n",
    "    return reshape_w,extra_w\n",
    "\n",
    "\n",
    "#just for fun, we are going to use the latent variables in a predictive model\n",
    "num_encodings = 5\n",
    "encoding_dim = 10\n",
    "X_prime = np.zeros((X.shape[0],num_encodings*encoding_dim))\n",
    "X_test_prime = np.zeros((X_test.shape[0],num_encodings*encoding_dim))\n",
    "\n",
    "#lets sample the last three iterations every 2 step (to avoid similarities)\n",
    "start = 0\n",
    "for i in np.arange(10,20,2):\n",
    "    w=parameter_by_run[i].copy()\n",
    "    X_prime[:,start:start+encoding_dim] = get_latent_encoding(X,w,link)\n",
    "    X_test_prime[:,start:start+encoding_dim] = get_latent_encoding(X_test,w,link)\n",
    "    start = start+encoding_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfsAAAFJCAYAAACciYSsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xt8FPXd//3X7CGbw25OEBCQIAGDYAIaUqyXAbUeoJ4K\nEYSkN+hPa1sUWrjUB71shXLV1vroBb/eDwJVq962XAUUwUO1lodFATkImoKBRFAChjMmJIRsSLJJ\ndu4/0ECUJBBCZnfyfv6DzM7Ofj67jO/Zme9+xzBN00RERERsy2F1ASIiInJxKexFRERsTmEvIiJi\ncwp7ERERm1PYi4iI2JzCXkRExOZcVhdwsZSWVnXo9hISoqmoONmh27SKXXqxSx+gXkKVXXqxSx+g\nXlqTlORr8TF9sz9HLpfT6hI6jF16sUsfoF5ClV16sUsfoF7aS2EvIiJicwp7ERERm1PYi4iI2JzC\nXkRExOYU9iIiIjansBcREbE5hb2IiIjNKexFRERs7pzC/pNPPmHy5MkAlJSUkJOTQ25uLnPmzCEY\nDALwyiuvkJ2dzT333MP7778PQG1tLdOnTyc3N5cHH3yQ8vJyALZt28aECROYNGkSeXl5Ta+Tl5fH\n+PHjmTRpEgUFBQCUl5dz//33k5uby4wZM6ipqem47kVERLqANsP+z3/+M7/61a+oq6sD4KmnnmLG\njBksWbIE0zRZvXo1paWlLF68mGXLlvHCCy8wf/58AoEAS5cuJTU1lSVLljB27FgWLVoEwJw5c5g3\nbx5Lly7lk08+oaioiMLCQrZs2cLy5cuZP38+c+fOBWDRokXccccdLFmyhCFDhvDyyy9fxLdDRETE\nftoM++TkZBYsWND098LCQkaMGAHAqFGj2LhxIwUFBVx99dVERETg8/lITk5m586d5OfnM3LkyKZ1\nN23ahN/vJxAIkJycjGEYZGVlsXHjRvLz88nKysIwDHr37k1jYyPl5eXf2sbGjRsvxvvQqh17jlFy\n5ESnv66IiEhHaPNGOKNHj+bAgQNNfzdNE8MwAIiJiaGqqgq/34/Pd3oC/piYGPx+f7PlZ67r9Xqb\nrbt//348Hg/x8fHNln9z218vOxcJCdEdMu+waZq8mLeBhsYg/3fm9VzSLeaCtxkKWrthQjixSx+g\nXkKVXXqxSx+gXtrjvO9653CcPhlQXV1NbGwsXq+X6urqZst9Pl+z5a2tGxsbi9vtbnUbkZGRTeue\ni468k9C4kf35/97Zydw/b+LxycOJjAjvmwUmJfk6/K6AVrBLH6BeQpVderFLH6Be2tpeS857NP6Q\nIUPYvHkzAOvWrSMzM5OhQ4eSn59PXV0dVVVVFBcXk5qaSkZGBmvXrm1ad/jw4Xi9XtxuN/v27cM0\nTdavX09mZiYZGRmsX7+eYDDIoUOHCAaDJCYmnnUbnW3ksN7cfl1/DpRW8+Lbn2KaZqfXICIi0l7n\n/RV11qxZPPHEE8yfP5+UlBRGjx6N0+lk8uTJ5ObmYpomM2fOxOPxkJOTw6xZs8jJycHtdjNv3jwA\n5s6dy6OPPkpjYyNZWVkMGzYMgMzMTCZOnEgwGGT27NkATJ06lVmzZvHKK6+QkJDQtI3O9qMfpPH5\nvgo+3lXKPz4s4fZrL7OkDhERkfNlmDb9mtrRp3mSknwUf3GM//7LR1ScqOPnE4YydED3Dn2NzmKX\n02B26QPUS6iySy926QPUS1vba4km1TkPsTERTMtOx+Vy8OybRRwp77hxASIiIheLwv48XXZJLPeN\nuYKaugYWrCigpq7B6pJERERapbBvh2vTLuHW7/Tl8LGTPP9WEUF7XgkRERGbUNi304QbBzC4XwJb\nPy/j7xu+sLocERGRFins28npcDB1bBrd4yJ5Y/1etn5WanVJIiIiZ6WwvwDeKDfTstOJcDt47q0i\nDpZVt/0kERGRTqawv0DJPX3cf9tg6gKN5K0o4GRtvdUliYiINKOw7wAjBvfktu/242hFDc++WUQw\nqAF7IiISOhT2HSR7VAppKYls33OM1z7YY3U5IiIiTRT2HcThMPjJXVfSIyGKtzeV8NHOL60uSURE\nBFDYd6iYSDfTs9PxRDh54e0i9n/pt7okERERhX1H65Pk5Ue3DyFQH2TBigL8NRqwJyIi1lLYXwTD\nByVx13WXUVZZyzNv7KAxGLS6JBER6cIU9hfJXVn9uWpgd4q+qODVNcVWlyMiIl2Ywv4icRgGD945\nhF7dolm1ZT+bCo9YXZKIiHRRCvuLKMrjYlp2OlEeJy+9s5OSI/a4B7OIiIQXhf1F1qtbDD++80oa\nGoIsWFnAieqA1SWJiEgXo7DvBMMGdmfsqBTKT9Txp9d30NCoAXsiItJ5FPad5I5r+zF8UBK79h/n\n5fd2W12OiIh0IQr7TmIYBg/cPpg+STGszj/ABwWHrC5JRES6CIV9J4qMcDE9O52YSBeLV+2i+FCl\n1SWJiEgXoLDvZD0SovnJD66kMWiycOV2Kv11VpckIiI2p7C3QFr/bky4YSDH/QEWvqYBeyIicnEp\n7C0yekRfrhnSk90HK1ny7mdWlyMiIjamsLeIYRjc9/0rSO7hZc22Q6zZetDqkkRExKYU9hbyuJ1M\nuzsdb5Sbv737GZ8fOG51SSIiYkMKe4t1j4ti6tg0TBMWvraDiioN2BMRkY6lsA8Bg/slMPGmgZyo\nDpC3cjv1DY1WlyQiIjaisA8RNw+/lOvSLmHv4RP8ddUuTNO0uiQREbEJhX2IMAyDKWMG0b+Xjw3b\nj/DevzVgT0REOobCPoS4XU4eHpdObLSbpf/6nJ0lFVaXJCIiNqCwDzGJsZE8NC4dw4BFr++grLLG\n6pJERCTMKexDUGrfeHJvScVfU0/eyu3U1WvAnoiItJ/CPkTdcFVvRg3rzb6jfv7yzk4N2BMRkXZT\n2IcowzD44S2pDOgTy4dFR1m1Zb/VJYmISJhS2Icwt8vBw+PSifNGsHzNbgr3lltdkoiIhCGFfYiL\n93qYNi4dp8PgmTd28OVxDdgTEZHzo7APAwP6xDH51kFU1zaQt6KA2kCD1SWJiEgYUdiHiZHDevO9\njD4cKK3mxbc/1YA9ERE5Zwr7MDLppstJ7RvPx7tK+ceHJVaXIyIiYUJhH0ZcTgcPjU0jMdbDyrV7\nKCgus7okEREJAwr7MBMbE8G07HRcLgfPvlnEkfKTVpckIiIhTmEfhi67JJb7xlxBTV0DC1YUUFOn\nAXsiItIyhX2YujbtEm79Tl8OHzvJ828VEdSAPRERaYHCPoxNuHEAg/slsPXzMv6+4QuryxERkRCl\nsA9jToeDqWPT6B4XyRvr97L1s1KrSxIRkRCksA9z3ig307LTiXA7eO6tIg6WVVtdkoiIhBiFvQ0k\n9/Rx/22DqQs0kreigJO19VaXJCIiIaRdYV9fX88jjzzCpEmTyM3Npbi4mJKSEnJycsjNzWXOnDkE\ng0EAXnnlFbKzs7nnnnt4//33AaitrWX69Onk5uby4IMPUl5+6gYv27ZtY8KECUyaNIm8vLym18vL\ny2P8+PFMmjSJgoKCC+3ZlkYM7slt3+3H0Yoann2ziGBQA/ZEROQUV3uetHbtWhoaGli2bBkbNmzg\nj3/8I/X19cyYMYNrrrmG2bNns3r1aq666ioWL17MihUrqKurIzc3l+uuu46lS5eSmprK9OnTefvt\nt1m0aBG/+tWvmDNnDgsWLKBv3778+Mc/pqioCNM02bJlC8uXL+fw4cNMnz6dFStWdPT7YAvZo1LY\n92UV2/cc47UP9nD39QOsLklEREJAu77Z9+/fn8bGRoLBIH6/H5fLRWFhISNGjABg1KhRbNy4kYKC\nAq6++moiIiLw+XwkJyezc+dO8vPzGTlyZNO6mzZtwu/3EwgESE5OxjAMsrKy2LhxI/n5+WRlZWEY\nBr1796axsbHpTIA053AY/OSuK+mREMXbm0r4aOeXVpckIiIhoF1hHx0dzcGDB/n+97/PE088weTJ\nkzFNE8MwAIiJiaGqqgq/34/P52t6XkxMDH6/v9nyM9f1er3N1m1tuZxdTKSb6dnpeCKcvPB2Efu/\n9FtdkoiIWKxdp/FfeuklsrKyeOSRRzh8+DD33nsv9fWnB4VVV1cTGxuL1+ulurq62XKfz9dseWvr\nxsbG4na7z7qNtiQkRONyOdvTXouSktp+3VCQlOTjkdwMfvfSRyx6fQfzZ1xPbEzEt9axA7v0Aeol\nVNmlF7v0AeqlPdoV9l+HMEBcXBwNDQ0MGTKEzZs3c80117Bu3Tq++93vMnToUP74xz9SV1dHIBCg\nuLiY1NRUMjIyWLt2LUOHDmXdunUMHz4cr9eL2+1m37599O3bl/Xr1zNt2jScTid/+MMfeOCBBzhy\n5AjBYJDExMQ2a6yo6Ng545OSfJSWhs8ZhYGX+Ljrust4c8MX/PbFD5l5zzCcjlMncsKtl5bYpQ9Q\nL6HKLr3YpQ9QL21tryXtCvv77ruPxx9/nNzcXOrr65k5cyZpaWk88cQTzJ8/n5SUFEaPHo3T6WTy\n5Mnk5uZimiYzZ87E4/GQk5PDrFmzyMnJwe12M2/ePADmzp3Lo48+SmNjI1lZWQwbNgyAzMxMJk6c\nSDAYZPbs2e0puUu6K6s/+4762ba7jFfXFDPxe5dbXZKIiFjAME17Tqre0Ud+4Xo0WVPXwJN//ZjD\nx07y4J1DuPbKS8K2l2+ySx+gXkKVXXqxSx+gXtraXks0qY7NRXlcTMtOJ8rj5KV3dlJyxB47iYiI\nnDuFfRfQq1sMP77zShoagixYWcDxqjqrSxIRkU6ksO8ihg3szthRKZSfqOPpxR/R0Bi0uiQREekk\nCvsu5I5r+zF8UBI7io/x8nu7rS5HREQ6icK+CzEMgwduH0y/S3yszj/ABwWHrC5JREQ6gcK+i4mM\ncPHL/3MNMZEuFq/aRfGhSqtLEhGRi0xh3wX16h7DT35wJY1Bk4Urt1Pp14A9ERE7U9h3UWn9uzHh\nhoEc9wdY+NoODdgTEbExhX0XNnpEX64Z0pPdBytZ8u5nVpcjIiIXicK+CzMMg/u+fwXJPbys2XaI\nNVsPWl2SiIhcBAr7Ls7jdjLt7nS8UW7+9u5nfH7guNUliYhIB1PYC93jopg6Ng3ThIWv7aBCM+yJ\niNiKwl4AGNwvgYk3DeREdYC8ldupb2i0uiQREekgCntpcvPwS7ku7RL2Hj7BX1ftwqY3RBQR6XIU\n9tLEMAymjBlE/14+Nmw/wnv/1oA9ERE7UNhLM26Xk4fHpRMb7Wbpvz5nZ0mF1SWJiMgFUtjLtyTG\nRvLQuHQMAxa9voOyyhqrSxIRkQugsJezSu0bT+4tqfhr6slbuZ26eg3YExEJVwp7adENV/Vm1LDe\n7Dvq5y/v7NSAPRGRMKWwlxYZhsEPb0llQJ9YPiw6yqot+60uSURE2kFhL61yuxw8PC6dOG8Ey9fs\npnBvudUliYjIeVLYS5vivR6mjUvH6TB45o0dfHlcA/ZERMKJwl7OyYA+cUy+dRDVtQ3krSigNtBg\ndUkiInKOFPZyzkYO6833MvpwoLSaF9/+VAP2RETChMJezsukmy4ntW88H+8q5R8fllhdjoiInAOF\nvZwXl9PBQ2PTSPB5WLl2DwXFZVaXJCIibVDYy3mLjYlg+t3puFwOnn2ziCPlJ60uSUREWqGwl3a5\n7JJY7htzBTV1DSxYUUBNnQbsiYiEKoW9tNu1aZdw63f6cvjYSZ5/q4igBuyJiIQkhb1ckAk3DmBw\nvwS2fl7G3zd8YXU5IiJyFgp7uSBOh4OpY9PoHhfJG+v3svWzUqtLEhGRb1DYywXzRrmZlp1OhNvB\nc28VcbCs2uqSRETkDAp76RDJPX3cf9tg6gKN5K0o4GRtvdUliYjIVxT20mFGDO7Jbd/tx9GKGp59\ns4hgUAP2RERCgcJeOlT2qBTSUhLZvucYr32wx+pyREQEhb10MIfD4Cd3XUmPhCje3lTCRzu/tLok\nEZEuT2EvHS4m0s307HQ8EU5eeLuI/V/6rS5JRKRLU9jLRdEnycuPbh9CoD7IghUF+Gs0YE9ExCoK\ne7lohg9K4q7rLqOsspZn3thBYzBodUkiIl2Swl4uqruy+nPVwO4UfVHBq2uKrS5HRKRLUtjLReUw\nDB68cwi9ukWzast+NhUesbokEZEuR2EvF12Ux8W07HSiPE5eemcnJUeqrC5JRKRLUdhLp+jVLYYf\n33klDQ1BFqws4ER1wOqSRES6DIW9dJphA7szdlQK5Sfq+NPrO2ho1IA9EZHOoLCXTnXHtf0YPiiJ\nXfuP8/J7u60uR0SkS1DYS6cyDIMHbh9Mn6QYVucf4IOCQ1aXJCJiewp76XSRES6mZ6cT7XGxeNUu\nig9VWl2SiIitKezFEj0Sovnp2CtpDJosXLmdSn+d1SWJiNiWwl4sk9a/GxNuGMhxf4CFr2nAnojI\nxeJq7xOfffZZ3nvvPerr68nJyWHEiBH84he/wDAMLr/8cubMmYPD4eCVV15h2bJluFwupk6dyo03\n3khtbS2PPfYYx44dIyYmhqeffprExES2bdvGb3/7W5xOJ1lZWUybNg2AvLw81qxZg8vl4vHHH2fo\n0KEd9gaItUaP6EvJ0So2Fx1lybufMWXMFVaXJCJiO+36Zr9582a2bt3K0qVLWbx4MUeOHOGpp55i\nxowZLFmyBNM0Wb16NaWlpSxevJhly5bxwgsvMH/+fAKBAEuXLiU1NZUlS5YwduxYFi1aBMCcOXOY\nN28eS5cu5ZNPPqGoqIjCwkK2bNnC8uXLmT9/PnPnzu3QN0CsZRgG933/CpJ7eFmz7RBrth60uiQR\nEdtpV9ivX7+e1NRUHn74YX76059yww03UFhYyIgRIwAYNWoUGzdupKCggKuvvpqIiAh8Ph/Jycns\n3LmT/Px8Ro4c2bTupk2b8Pv9BAIBkpOTMQyDrKwsNm7cSH5+PllZWRiGQe/evWlsbKS8vLzj3gGx\nnMftZNrd6Xij3Pzt3c/4/MBxq0sSEbGVdp3Gr6io4NChQzzzzDMcOHCAqVOnYpomhmEAEBMTQ1VV\nFX6/H5/P1/S8mJgY/H5/s+Vnruv1eputu3//fjweD/Hx8c2WV1VVkZiY2GqNCQnRuFzO9rTXoqQk\nX9srhYlQ6yUpycd/3fcdnnh2E396o5A/zryebnFR5/Q8u1AvockuvdilD1Av7dGusI+PjyclJYWI\niAhSUlLweDwcOXL6BifV1dXExsbi9Xqprq5uttzn8zVb3tq6sbGxuN3us26jLRUVJ9vTWouSknyU\nltpjTvdQ7aVXXCQTvzeQpf/6nLl//pBf/PBq3K0csIVqH+2hXkKTXXqxSx+gXtraXkvadRp/+PDh\nfPDBB5imydGjR6mpqeHaa69l8+bNAKxbt47MzEyGDh1Kfn4+dXV1VFVVUVxcTGpqKhkZGaxdu7Zp\n3eHDh+P1enG73ezbtw/TNFm/fj2ZmZlkZGSwfv16gsEghw4dIhgMtvmtXsLXzcMv5bq0S9h7+AR/\nXbUL0zStLklEJOy165v9jTfeyEcffcT48eMxTZPZs2dz6aWX8sQTTzB//nxSUlIYPXo0TqeTyZMn\nk5ubi2mazJw5E4/HQ05ODrNmzSInJwe32828efMAmDt3Lo8++iiNjY1kZWUxbNgwADIzM5k4cSLB\nYJDZs2d3XPcScgzDYMqYQRw6Vs2G7Ufo19PHzZl9rS5LRCSsGaZNvzp19GkenTrqXOUnavnvlz7C\nX9PAo5Ou4op+Cd9aJxz6OFfqJTTZpRe79AHqpa3ttUST6khISoyN5KFx6RgGLHp9B2WVNVaXJCIS\nthT2ErJS+8aTe0sq/pp68lZup66+0eqSRETCksJeQtoNV/Vm1LDe7Dvq5y/v7NSAPRGRdlDYS0gz\nDIMf3pLKgD6xfFh0lFVb9ltdkohI2FHYS8hzuxw8PC6dOG8Ey9fspnCvZlAUETkfCnsJC/FeD9PG\npeN0GDzzxg6+PK4BeyIi50phL2FjQJ84Jt86iOraBvJWFFBT12B1SSIiYUFhL2Fl5LDefC+jDwdK\nq/l/l23VgD0RkXOgsJewM+mmy0ntG8+GgkP848MSq8sREQl5CnsJOy6ng4fGptE9LpKVa/dQUFxm\ndUkiIiFNYS9hKTYmgl/+n2twuRw8+2YRR8o79i6HIiJ2orCXsDWwbzz3jbmCmroGFmjAnohIixT2\nEtauTbuEW7/Tl8PHTvL8W0UENWBPRORbFPYS9ibcOIDB/RLY+nkZf9/whdXliIiEHIW9hD2nw8HU\nrwbsvbF+L1s/K7W6JBGRkKKwF1vwRrmZlp1OhNvBc28VcbCs2uqSRERChsJebCO5p4/7bxtMXaCR\nvBUFnKytt7okEZGQoLAXWxkxuCe3fbcfRytqePbNIoJBDdgTEVHYi+1kj0ohLSWR7XuO8doHe6wu\nR0TEcgp7sR2Hw+And11Jj4Qo3t5UwpZPj1pdkoiIpRT2YksxkW6mZ6fjiXDy4j8+Zf+XfqtLEhGx\njMJebKtPkpcf3T6EQH2QBSsK8NdowJ6IdE0Ke7G14YOSuOu6yyirrOWZN3bQGAxaXZKISKdT2Ivt\n3ZXVn6sGdqfoiwpeXVNsdTkiIp1OYS+25zAMHrxzCL26RbNqy342FR6xuiQRkU6lsJcuIcrjYlp2\nOlEeJy+9s5OSI1VWlyQi0mkU9tJl9OoWw4/vvJKGhiALVhZwojpgdUkiIp1CYS9dyrCB3Rk7KoXy\nE3X86fUdNDRqwJ6I2J/CXrqcO67tx/BBSezaf5yX39ttdTkiIhedwl66HMMweOD2wfRJimF1/gE+\nKDhkdUkiIheVwl66pMgIF9Oz04n2uFi8ahfFhyqtLklE5KJR2EuX1SMhmp/+4EoagyYLV26n0l9n\ndUkiIheFwl66tLSUboy/YQDH/QEWvqYBeyJiTwp76fLGjEjmmiE92X2wkiXvfmZ1OSIiHU5hL12e\nYRjc9/0rSO7hZc22Q6zZetDqkkREOpTCXgTwuJ1Muzsdb5Sbv737GZ8fOG51SSIiHUZhL/KV7nFR\nTB2bhmnCwtd2UFGlAXsiYg8Ke5EzDO6XwMSbBnKiOkDeyu3UNzRaXZKIyAVT2It8w83DL+W6tEvY\ne/gEf121C9M0rS5JROSCKOxFvsEwDKaMGUT/Xj42bD/C6vwDVpckInJBFPYiZ+F2OXl4XDqx0W6W\nrd7NzpIKq0sSEWk3hb1ICxJjI3loXDqGAYte30FZZY3VJYmItIvCXqQVqX3jyb0lFX9NPXkrt1NX\nrwF7IhJ+FPYibbjhqt6MGtabfUf9/OWdnRqwJyJhR2Ev0gbDMPjhLakM6BPLh0VHWbVlv9UliYic\nF4W9yDlwuxw8PC6dOG8Ey9fspnBvudUliYicM4W9yDmK93qYNi4dp8PgmTd28OVxDdgTkfCgsBc5\nDwP6xDH51kFU1zaQt6KA2kCD1SWJiLTpgsL+2LFjXH/99RQXF1NSUkJOTg65ubnMmTOHYPDUfcFf\neeUVsrOzueeee3j//fcBqK2tZfr06eTm5vLggw9SXn7qlOi2bduYMGECkyZNIi8vr+l18vLyGD9+\nPJMmTaKgoOBCSha5YCOH9eZ7GX04UFrNi29/qgF7IhLy2h329fX1zJ49m8jISACeeuopZsyYwZIl\nSzBNk9WrV1NaWsrixYtZtmwZL7zwAvPnzycQCLB06VJSU1NZsmQJY8eOZdGiRQDMmTOHefPmsXTp\nUj755BOKioooLCxky5YtLF++nPnz5zN37tyO6VzkAky66XJS+8bz8a5S/vFhidXliIi0qt1h//TT\nTzNp0iR69OgBQGFhISNGjABg1KhRbNy4kYKCAq6++moiIiLw+XwkJyezc+dO8vPzGTlyZNO6mzZt\nwu/3EwgESE5OxjAMsrKy2LhxI/n5+WRlZWEYBr1796axsbHpTICIVVxOBw+NTSPB52Hl2j0UFJdZ\nXZKISItc7XnSypUrSUxMZOTIkTz33HMAmKaJYRgAxMTEUFVVhd/vx+fzNT0vJiYGv9/fbPmZ63q9\n3mbr7t+/H4/HQ3x8fLPlVVVVJCYmtlpjQkI0LpezPe21KCnJ1/ZKYcIuvVjZR1ISzH7gu8zK+4A/\n/72IeTOup0+St+0ntrg9e3wmoF5CkV36APXSHu0K+xUrVmAYBps2beLTTz9l1qxZzb5tV1dXExsb\ni9frpbq6utlyn8/XbHlr68bGxuJ2u8+6jbZUVJxsT2stSkryUVpa1aHbtIpdegmFPuIinUwZM4jn\n3/qUuX/exK+mZBLlOf/dKhR66SjqJfTYpQ9QL21tryXtOo3/t7/9jf/93/9l8eLFDB48mKeffppR\no0axefNmANatW0dmZiZDhw4lPz+furo6qqqqKC4uJjU1lYyMDNauXdu07vDhw/F6vbjdbvbt24dp\nmqxfv57MzEwyMjJYv349wWCQQ4cOEQwG2/xWL9KZ/iOtF7d+py+Hj53k+beKCGrAnoiEmHZ9sz+b\nWbNm8cQTTzB//nxSUlIYPXo0TqeTyZMnk5ubi2mazJw5E4/HQ05ODrNmzSInJwe32828efMAmDt3\nLo8++iiNjY1kZWUxbNgwADIzM5k4cSLBYJDZs2d3VMkiHWbCjQPY/6WfrZ+X8fcNX/CDrP5WlyQi\n0sQwbfq7oY4+zaNTR6En1Prw19Tz3y99RFllLdOz07k6NemcnxtqvVwI9RJ67NIHqJe2ttcSTaoj\n0kG8UW6mZacT4Xbw3FtFHCyrbvtJIiKdQGEv0oGSe/q4/7bB1AUayVtRwMnaeqtLEhFR2It0tBGD\ne3Lbd/txtKKGZ98sIhi05ZUyEQkjCnuRiyB7VAppKYls33OM1z7YY3U5ItLFKexFLgKHw+And11J\nj4Qo3t5UwpZPj1pdkoh0YQp7kYskJtLN9Ox0PBFOXvzHp+z/0m91SSLSRSnsRS6iPklefnT7EAL1\nQRasKMBfowF7ItL5FPYiF9nwQUncdd1llFXW8swbO2j86vbPIiKdRWEv0gnuyurPVQO7U/RFBa+u\nKba6HBHpYhT2Ip3AYRg8eOcQenWLZtWW/WwqPGJ1SSLShSjsRTpJlMfFtOx0ojxOXnpnJyVH7DHl\np4iEPoVu1X8kAAAUpElEQVS9SCfq1S2GH995JQ0NQRasLOBEdcDqkkSkC1DYi3SyYQO7M3ZUCuUn\n6vjT6ztoaNSAPRG5uBT2Iha449p+DB+UxK79x3n5vd1WlyMiNqewF7GAYRg8cPtg+iTFsDr/AP/a\nUmJ1SSJiYwp7EYtERriYnp1OtMfFwlcLKD5UaXVJImJTCnsRC/VIiOanP7iSYDDIwpXbqfTXWV2S\niNiQwl7EYmkp3bj39iEc9wdY+JoG7IlIx1PYi4SAcTcM5JohPdl9sJIl735mdTkiYjMKe5EQYBgG\n933/CpJ7eFmz7RBrth60uiQRsRGFvUiI8LidTLs7HW+Um7+9+xmfHzhudUkiYhMKe5EQ0j0uiqlj\n0zBNWPjaDiqqNGBPRC6cwl4kxAzul8DEmwZyojpA3soC6hsarS5JRMKcwl4kBN08/FKuS7uEvYer\n+Os/d2GaptUliUgYU9iLhCDDMJgyZhD9e/nYsOMIq/MPWF2SiIQxhb1IiHK7nDw8Lp3YaDfLVu9m\nZ0mF1SWJSJhS2IuEsMTYSB4al45hwKLXd1BWWWN1SSIShhT2IiEutW88ubek4q+pJ2/ldurqNWBP\nRM6Pwl4kDNxwVW9GDevNvqN+/vLOTg3YE5HzorAXCQOGYfDDW1IZ0CeWD4uOsmrLfqtLEpEworAX\nCRNul4OHx6UT541g+ZrdFO4tt7okEQkTCnuRMBLv9TBtXDpOh8Ezb+zgy+MasCcibVPYi4SZAX3i\nmHzrIKprG8hbUUBtoMHqkkQkxCnsRcLQyGG9+V5GHw6UVvPi259qwJ6ItEphLxKmJt10Oal94/l4\nVyn/+LDE6nJEJIQp7EXClMvp4KGxaST4PKxcu4eC4jKrSxKREKWwFwljsTERTL87HZfLwbNvFnGk\n/KTVJYlICFLYi4S5yy6J5d4xg6ipa2DBigJq6jRgT0SaU9iL2MB/pPXi1u/05fCxkzz/VhFBDdgT\nkTMo7EVsYsKNAxjcL4Gtn5fx9w1fWF2OiIQQhb2ITTgdDqaOTaN7XCRvrN/L1s9KrS5JREKEwl7E\nRrxRbqZlpxPhdvDcW0UcLKu2uiQRCQEKexGbSe7p4/7bBlMXaGTBigJO1tZbXZKIWExhL2JDIwb3\n5Lbv9uPLihqefbOIYFAD9kS6MoW9iE1lj0ohLSWR7XuO8doHe6wuR0QspLAXsSmHw+And11Jj4Qo\n3t5UwpZPj1pdkohYRGEvYmMxkW6mZ6fjiXDy4j8+Zf+XfqtLEhELKOxFbK5Pkpcf3T6EQH2QBSsK\n8NdowJ5IV6OwF+kChg9K4q7rLqOsspZn3thBYzBodUki0olc7XlSfX09jz/+OAcPHiQQCDB16lQG\nDhzIL37xCwzD4PLLL2fOnDk4HA5eeeUVli1bhsvlYurUqdx4443U1tby2GOPcezYMWJiYnj66adJ\nTExk27Zt/Pa3v8XpdJKVlcW0adMAyMvLY82aNbhcLh5//HGGDh3aoW+CSFdwV1Z/9h31s213Ga+u\nKWbi9y63uiQR6STt+mb/5ptvEh8fz5IlS3j++ef5zW9+w1NPPcWMGTNYsmQJpmmyevVqSktLWbx4\nMcuWLeOFF15g/vz5BAIBli5dSmpqKkuWLGHs2LEsWrQIgDlz5jBv3jyWLl3KJ598QlFREYWFhWzZ\nsoXly5czf/585s6d26FvgEhX4TAMHrxzCL26RbNqy342FR6xuiQR6STtCvsxY8bw85//HADTNHE6\nnRQWFjJixAgARo0axcaNGykoKODqq68mIiICn89HcnIyO3fuJD8/n5EjRzatu2nTJvx+P4FAgOTk\nZAzDICsri40bN5Kfn09WVhaGYdC7d28aGxspLy/voPZFupYoj4tp2elEeZy89M5OSo5UWV2SiHSC\ndp3Gj4mJAcDv9/Ozn/2MGTNm8PTTT2MYRtPjVVVV+P1+fD5fs+f5/f5my89c1+v1Nlt3//79eDwe\n4uPjmy2vqqoiMTGx1RoTEqJxuZztaa9FSUm+tlcKE3bpxS59QOf1kpTk47H/J5PfvLiZha/v4P/O\nuJ54n6fDX8Mu7NKLXfoA9dIe7Qp7gMOHD/Pwww+Tm5vLnXfeyR/+8Iemx6qrq4mNjcXr9VJdXd1s\nuc/na7a8tXVjY2Nxu91n3UZbKipOtre1s0pK8lFaao9vQXbpxS59QOf3cllSDGNHpvDauj08+cKH\nPDLpKlzOjhmvq88l9NilD1AvbW2vJe3au8vKyrj//vt57LHHGD9+PABDhgxh8+bNAKxbt47MzEyG\nDh1Kfn4+dXV1VFVVUVxcTGpqKhkZGaxdu7Zp3eHDh+P1enG73ezbtw/TNFm/fj2ZmZlkZGSwfv16\ngsEghw4dIhgMtvmtXkTadse1/Rg+KIld+4/z8nu7rS5HRC6idn2zf+aZZzhx4gSLFi1qGlz3y1/+\nkieffJL58+eTkpLC6NGjcTqdTJ48mdzcXEzTZObMmXg8HnJycpg1axY5OTm43W7mzZsHwNy5c3n0\n0UdpbGwkKyuLYcOGAZCZmcnEiRMJBoPMnj27g1oX6doMw+CB2wdzpPwkq/MPkNzTy8ihva0uS0Qu\nAsM0TVveIaOjT/Po1FHosUsfYG0vX1ac5L9f+phAQyOzfpjBgN5xF7Q9fS6hxy59gHppa3st0aQ6\nIl1cj4RofvqDK2kMmixcuZ1Kf53VJYlIB1PYiwhpKd0Yf8MAjvsDLHxtBw2NmmFPxE4U9iICwJgR\nyVwzpCe7D1ay5N3PrC5HRDqQwl5EgFMD9u77/hUk9/CyZtsh1mw9aHVJItJBFPYi0sTjdjLt7nS8\nUW7+9u5nfH7guNUliUgHUNiLSDPd46KYOjYN04SFr+2g/ESt1SWJyAVS2IvItwzul8DEmwZyojrA\nwte2U9/QaHVJInIBFPYiclY3D7+U69IuYe/hKv76z13YdEoOkS5BYS8iZ2UYBlPGDKJ/Lx8bdhxh\ndf4Bq0sSkXZS2ItIi9wuJw+PSyc22s2y1bvZWVJhdUki0g4KexFpVWJsJA+NS8cwYNHrOyirrLG6\nJBE5Twp7EWlTat94cm9JxV9TT97K7dTVa8CeSDhR2IvIObnhqt6MGtabfUf9/OWdnRqwJxJGFPYi\nck4Mw+CHt6QyoE8sHxYdZdWW/VaXJCLnSGEvIufM7XLw8Lh04rwRLF+zm8K95VaXJCLnQGEvIucl\n3uth2rh0nA6DZ97YwZfHNWBPJNQp7EXkvA3oE8fkWwdRXdtA3ooCagMNVpckIq1Q2ItIu4wc1pvv\nZfThQGk1L779qQbsiYQwhb2ItNukmy4ntW88H+8q5R8fllhdjoi0QGEvIu3mcjp4aGwaCT4PK9fu\noaC4zOqSROQsFPYickFiYyKYfnc6LpeDZ98sYsMnh9h7+AQVVXU0BoNWlycigMvqAkQk/F12SSz3\njhnE8299yu//+lHTcgPwRruJi/EQ740gzhtBvNdDXEwEcV/9eWq5B4/baV0DIjansBeRDvEfab1I\n8EVSVlXHwaNVHPfXUekPUFkd4NiJGg6U+lt9fpTH2XRQEBvz1UGBN4L4mFN/xnlPPRbtcWEYRid1\nJWIPCnsR6TCD+yWQlOSjtLTqW4/VBRqprK7j+FcHAKcPBk79efyr/z5SfrLV13A5Hc3OCJw6IDjz\nTMGpZbHRETgcOigQAYW9iHQST4STHhHR9EiIbnW9hsYgJ6pPHRBU+gMc/+pgoNJf13RAcNwf4Isj\nVTQGT7S4HcOA2OjTlw5ivz5AaLqk4PnqICECt0uXEMTeFPYiElJcTgeJsZEkxka2ul7QNKmuqW92\nQHDmpYNKfx3HqwMcLa9h39HWLyFEe1ynxxOcceng0l5xOILBpoOEKI9TlxAkLCnsRSQsOQwDX3QE\nvugILsXb6ro1dQ1NBwCnLiE0P1Pw9YHC4WOtX0KIcDmaxg/ExZw5nuD0wMN4rwdvtBuHDgokhCjs\nRcT2ojwuojwuLkls/RJCfcOpSwhfnyloNAwOHD5x+oDgqwOGPQdPEGxlxkCHYRAb4z7jUsHpSwdx\nX106+PpAweXUL6Dl4lPYi4h8xe1y0C0ukm5xpy4htDTYMBg0qaqp/9bZgW9eUjhUVk3JkW8//0ze\nKHfTIMPYM8cTeCOaDTiMjND/rqX99K9HROQ8ORzGqW/oMREk92x5PdM0qak741cIZ7l0UFkdoOJE\nHQdLq1t9TY/b2fyXB83mLDh9psAb5da4AvkWhb2IyEViGAbRkS6iI1306hbT6rqB+sbTv0D46iDg\n9EHC6QGHn1dU0toth5wO49Q4gpjTkxb17uHD5aDpgCDe68EX7dYlhC5EYS8iEgIi3E6S4qNIio9q\ndb3GYJCqk/XNDgq+/hVC00GCP8D+L6vY29jyYYFmN+xaFPYiImHE6XAQ7/UQ7/XQD1+L65mmSXVt\nA5X+OnA5KTl4vOlniU0HB9UByio7ZnbDuJgIYiI1u2GoUtiLiNiQYRh4o9x4o9wkJfnok9DyGYO6\nQOPpyYvOnN3wq0sHX/9kUbMbhi+FvYhIF+eJcNIzIpqe5zG74ZmXDk5UBzS7YYhT2IuIyDk5n9kN\n/V/Nblj5jdkNm84U+E+dKTjf2Q17JfmIcNA0tkCzG54bhb2IiHQoh2EQG33qdH3f85jd8HizSwct\nzW549Kzb0eyGrVPYi4iIZc5ndsPK6joMt4uSA8e/dWOkryc0Kj5YSSuTG7Y8u+GZ8xfYcHZDhb2I\niIQ8t8tB97gokpJ8dIt2t7jet2Y3PHOQ4RkDEM9ndsNTEyidfXbD2JgIojyhH6WhX6GIiMg5Or/Z\nDRtavXRQWR2g3CazGyrsRUSkyzk1u6Gb6Eg3vbuf/+yGpwcc1nHiq4GH5zO7YYLPw313XonX3TmX\nChT2IiIirTif2Q1PVNc3uxfCt6ZA9tedmt3w8An+Y18FGQO6dUoPCnsREZEO4HQ4SPCd+tbeGtM0\nCdQHubRP/Fnvqngx2GeooYiISBgwDANPROdOFqSwFxERsTmFvYiIiM0p7EVERGxOYS8iImJzYTEa\nPxgM8utf/5pdu3YRERHBk08+Sb9+/awuS0REJCyExTf7f/3rXwQCAV5++WUeeeQRfv/731tdkoiI\nSNgIi7DPz89n5MiRAFx11VXs2LHD4opERETCR1icxvf7/Xi9p2+T6HQ6aWhowOVqufyEhGhcro79\nHWNSkq9Dt2clu/Rilz5AvYQqu/Rilz5AvbRHWIS91+uluvr0jQiCwWCrQQ9QUXGy1cfPV1KSr9Nm\nOrrY7NKLXfoA9RKq7NKLXfoA9dLW9loSFqfxMzIyWLduHQDbtm0jNTXV4opERETCR1h8s7/lllvY\nsGEDkyZNwjRNfve731ldkoiISNgwTNNs7Y58IiIiEubC4jS+iIiItJ/CXkRExOYU9iIiIjansBcR\nEbE5hb2IiIjNKexFRERsLix+Z38xtXVHvffee4+FCxficrm4++67ueeee0L2Lnxt1fXWW2/xl7/8\nBafTSWpqKr/+9a9xOByMGzeuaTriSy+9lKeeesqqFpq01ctLL73E8uXLSUxMBGDu3LlcdtllYfe5\nlJaW8p//+Z9N63766ac88sgj5OTkhOTnAvDJJ5/wP//zPyxevLjZ8nDaV77WUi/htK98raVewmlf\ngbP3EW77SX19PY8//jgHDx4kEAgwdepUbrrppqbHLdlXzC5u1apV5qxZs0zTNM2tW7eaP/3pT5se\nCwQC5s0332weP37crKurM7Ozs83S0tJWn2Ol1uqqqakxb7rpJvPkyZOmaZrmzJkzzX/9619mbW2t\n+YMf/MCSelvT1nv8yCOPmNu3bz+v51jlXOv697//bU6ePNlsaGgI2c/lueeeM++44w5zwoQJzZaH\n275imi33Em77imm23Itphte+0lofXwuH/eTVV181n3zySdM0TbOiosK8/vrrmx6zal/p8qfxW7uj\nXnFxMcnJycTFxREREcHw4cP56KOPQvYufK3VFRERwbJly4iKigKgoaEBj8fDzp07qamp4f7772fK\nlCls27bNktq/qa33uLCwkOeee46cnByeffbZc3qOVc6lLtM0+c1vfsOvf/1rnE5nyH4uycnJLFiw\n4FvLw21fgZZ7Cbd9BVruBcJrX2mtDwif/WTMmDH8/Oc/B07V7HSevimbVftKlz+N39od9fx+Pz7f\n6RsLxMTE4Pf723UXvs7QWl0Oh4Pu3bsDsHjxYk6ePMl1113HZ599xgMPPMCECRP44osvePDBB/nn\nP/8Z0r0A3H777eTm5uL1epk2bRrvv/9+WH4uX3vvvfe4/PLLSUlJASAyMjIkP5fRo0dz4MCBby0P\nt30FWu4l3PYVaLkXCK99pbU+IHz2k5iYGODUfvGzn/2MGTNmND1m1b5i/b9Si7V2R71vPlZdXY3P\n52vXXfg6Q1t1BYNB/vCHP7B3714WLFiAYRj079+ffv36Nf13fHw8paWl9OrVy4oWmrTWi2ma3Hvv\nvU07zPXXX09RUVHYfi4Ab775JlOmTGn6e6h+Li0Jt32lLeG0r7Qm3PaVtoTTfnL48GEefvhhcnNz\nufPOO5uWW7WvdPnT+K3dUW/AgAGUlJRw/PhxAoEAH3/8MVdffXXI3oWvrbpmz55NXV0dixYtajpF\n+eqrr/L73/8egKNHj+L3+0lKSurcws+itV78fj933HEH1dXVmKbJ5s2bSUtLC9vPBWDHjh1kZGQ0\n/T1UP5eWhNu+0pZw2ldaE277SlvCZT8pKyvj/vvv57HHHmP8+PHNHrNqXwn9Q7mL7Gx31Pv73//O\nyZMnmThxIr/4xS944IEHME2Tu+++m549e4bsXfha6yUtLY1XX32VzMxM7r33XgCmTJnC+PHj+a//\n+i9ycnIwDIPf/e53IXGE39bnMnPmTKZMmUJERATXXnst119/PcFgMOw+l4kTJ1JeXo7X68UwjKbn\nhOrn8k3huq+cTbjuK2cTrvvKN4XrfvLMM89w4sQJFi1axKJFiwCYMGECNTU1lu0ruuudiIiIzXX5\n0/giIiJ2p7AXERGxOYW9iIiIzSnsRUREbE5hLyIiYnMKexEREZtT2IuIiNicwl5ERMTm/n/YNiXo\nkbejigAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1133d6390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#lets combine the latent variables with PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X_prime)\n",
    "    \n",
    "plt.plot(pca.explained_variance_)\n",
    "plt.show()\n",
    "\n",
    "X_prime = pca.transform(X_prime)\n",
    "X_test_prime = pca.transform(X_test_prime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add the original variables to the features set\n",
    "X_prime = np.column_stack((X,X_prime))\n",
    "X_test_prime = np.column_stack((X_test,X_test_prime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Flatten\n",
    "from keras.models import Model, Sequential\n",
    "\n",
    "def get_acc(predict,y):\n",
    "    return np.sum((predict>0.5).astype(np.int)==y)/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124999 samples, validate on 125001 samples\n",
      "Epoch 1/100\n",
      "124999/124999 [==============================] - 1s 6us/step - loss: 0.5914 - val_loss: 0.5757\n",
      "Epoch 2/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5642 - val_loss: 0.5559\n",
      "Epoch 3/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.5540 - val_loss: 0.5555\n",
      "Epoch 4/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5422 - val_loss: 0.5354\n",
      "Epoch 5/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5348 - val_loss: 0.5273\n",
      "Epoch 6/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5238 - val_loss: 0.5204\n",
      "Epoch 7/100\n",
      "124999/124999 [==============================] - 0s 4us/step - loss: 0.5146 - val_loss: 0.5189\n",
      "Epoch 8/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5045 - val_loss: 0.5003\n",
      "Epoch 9/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4994 - val_loss: 0.5028\n",
      "Epoch 10/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4913 - val_loss: 0.4899\n",
      "Epoch 11/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4846 - val_loss: 0.4884\n",
      "Epoch 12/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4796 - val_loss: 0.4911\n",
      "Epoch 13/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4749 - val_loss: 0.4725\n",
      "Epoch 14/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4731 - val_loss: 0.4802\n",
      "Epoch 15/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4684 - val_loss: 0.4770\n",
      "Epoch 16/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4625 - val_loss: 0.4678\n",
      "Epoch 17/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4618 - val_loss: 0.4663\n",
      "Epoch 18/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4576 - val_loss: 0.4627\n",
      "Epoch 19/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4547 - val_loss: 0.4575\n",
      "Epoch 20/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4500 - val_loss: 0.4579\n",
      "Epoch 21/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4489 - val_loss: 0.4582\n",
      "Epoch 22/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4475 - val_loss: 0.4568\n",
      "Epoch 23/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4439 - val_loss: 0.4552\n",
      "Epoch 24/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4426 - val_loss: 0.4522\n",
      "Epoch 25/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4408 - val_loss: 0.4483\n",
      "Epoch 26/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4386 - val_loss: 0.4426\n",
      "Epoch 27/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4362 - val_loss: 0.4441\n",
      "Epoch 28/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4346 - val_loss: 0.4441\n",
      "Epoch 29/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4335 - val_loss: 0.4419\n",
      "Epoch 30/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4318 - val_loss: 0.4388\n",
      "Epoch 31/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4302 - val_loss: 0.4413\n",
      "Epoch 32/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4285 - val_loss: 0.4358\n",
      "Epoch 33/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4277 - val_loss: 0.4377\n",
      "Epoch 34/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4262 - val_loss: 0.4338\n",
      "Epoch 35/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4247 - val_loss: 0.4324\n",
      "Epoch 36/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4238 - val_loss: 0.4292\n",
      "Epoch 37/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4223 - val_loss: 0.4309\n",
      "Epoch 38/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4211 - val_loss: 0.4304\n",
      "Epoch 39/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4201 - val_loss: 0.4287\n",
      "Epoch 40/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4194 - val_loss: 0.4271\n",
      "Epoch 41/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4186 - val_loss: 0.4289\n",
      "Epoch 42/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4175 - val_loss: 0.4268\n",
      "Epoch 43/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4169 - val_loss: 0.4252\n",
      "Epoch 44/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4162 - val_loss: 0.4264\n",
      "Epoch 45/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4158 - val_loss: 0.4252\n",
      "Epoch 46/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4144 - val_loss: 0.4237\n",
      "Epoch 47/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4144 - val_loss: 0.4231\n",
      "Epoch 48/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4136 - val_loss: 0.4214\n",
      "Epoch 49/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4126 - val_loss: 0.4210\n",
      "Epoch 50/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4114 - val_loss: 0.4190\n",
      "Epoch 51/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4114 - val_loss: 0.4192\n",
      "Epoch 52/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4107 - val_loss: 0.4175\n",
      "Epoch 53/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4103 - val_loss: 0.4167\n",
      "Epoch 54/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4095 - val_loss: 0.4153\n",
      "Epoch 55/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4086 - val_loss: 0.4157\n",
      "Epoch 56/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4083 - val_loss: 0.4165\n",
      "Epoch 57/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4079 - val_loss: 0.4151\n",
      "Epoch 58/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4072 - val_loss: 0.4145\n",
      "Epoch 59/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4068 - val_loss: 0.4143\n",
      "Epoch 60/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4064 - val_loss: 0.4140\n",
      "Epoch 61/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4062 - val_loss: 0.4149\n",
      "Epoch 62/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4053 - val_loss: 0.4139\n",
      "Epoch 63/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4055 - val_loss: 0.4135\n",
      "Epoch 64/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4045 - val_loss: 0.4107\n",
      "Epoch 65/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4043 - val_loss: 0.4118\n",
      "Epoch 66/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4036 - val_loss: 0.4134\n",
      "Epoch 67/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4031 - val_loss: 0.4110\n",
      "Epoch 68/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4025 - val_loss: 0.4105\n",
      "Epoch 69/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4018 - val_loss: 0.4094\n",
      "Epoch 70/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4016 - val_loss: 0.4110\n",
      "Epoch 71/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4012 - val_loss: 0.4079\n",
      "Epoch 72/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4010 - val_loss: 0.4107\n",
      "Epoch 73/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4006 - val_loss: 0.4098\n",
      "Epoch 74/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4003 - val_loss: 0.4092\n",
      "Epoch 75/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3999 - val_loss: 0.4090\n",
      "Epoch 76/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3997 - val_loss: 0.4095\n",
      "Epoch 77/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3995 - val_loss: 0.4090\n",
      "Epoch 78/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3991 - val_loss: 0.4085\n",
      "Epoch 79/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3987 - val_loss: 0.4087\n",
      "Epoch 80/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3985 - val_loss: 0.4080\n",
      "Epoch 81/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3981 - val_loss: 0.4079\n",
      "Epoch 82/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3978 - val_loss: 0.4072\n",
      "Epoch 83/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3977 - val_loss: 0.4076\n",
      "Epoch 84/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3975 - val_loss: 0.4066\n",
      "Epoch 85/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3970 - val_loss: 0.4068\n",
      "Epoch 86/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3967 - val_loss: 0.4065\n",
      "Epoch 87/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3963 - val_loss: 0.4060\n",
      "Epoch 88/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3962 - val_loss: 0.4055\n",
      "Epoch 89/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3960 - val_loss: 0.4050\n",
      "Epoch 90/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3956 - val_loss: 0.4050\n",
      "Epoch 91/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3955 - val_loss: 0.4052\n",
      "Epoch 92/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3952 - val_loss: 0.4045\n",
      "Epoch 93/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3949 - val_loss: 0.4040\n",
      "Epoch 94/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3946 - val_loss: 0.4041\n",
      "Epoch 95/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3943 - val_loss: 0.4037\n",
      "Epoch 96/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3941 - val_loss: 0.4035\n",
      "Epoch 97/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3939 - val_loss: 0.4027\n",
      "Epoch 98/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3935 - val_loss: 0.4024\n",
      "Epoch 99/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3934 - val_loss: 0.4024\n",
      "Epoch 100/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.3932 - val_loss: 0.4023\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x118e4a860>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model with latent variable PCA components\n",
    "input_data = Input(shape=(X_prime.shape[1],))\n",
    "layer1 = Dense(200, activation='sigmoid')(input_data)\n",
    "layer2 = Dense(1, activation='sigmoid')(layer1)\n",
    "model = Model(input_data, layer2)\n",
    "\n",
    "model.compile(optimizer='adagrad', \n",
    "                    loss='binary_crossentropy')\n",
    "model.fit(X_prime, y,\n",
    "                epochs=100,\n",
    "                batch_size=5000,\n",
    "                shuffle=False,\n",
    "                validation_data=(X_test_prime, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.81968944248446007, 0.82370258962071696)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test = model.predict(X_test_prime)\n",
    "p_train = model.predict(X_prime)\n",
    "get_acc(p_test,y_test),get_acc(p_train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 124999 samples, validate on 125001 samples\n",
      "Epoch 1/100\n",
      "124999/124999 [==============================] - 1s 5us/step - loss: 0.5812 - val_loss: 0.5611\n",
      "Epoch 2/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5593 - val_loss: 0.5587\n",
      "Epoch 3/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5517 - val_loss: 0.5478\n",
      "Epoch 4/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5480 - val_loss: 0.5486\n",
      "Epoch 5/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5437 - val_loss: 0.5420\n",
      "Epoch 6/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5404 - val_loss: 0.5393\n",
      "Epoch 7/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5380 - val_loss: 0.5367\n",
      "Epoch 8/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5351 - val_loss: 0.5340\n",
      "Epoch 9/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5332 - val_loss: 0.5350\n",
      "Epoch 10/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5302 - val_loss: 0.5328\n",
      "Epoch 11/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5277 - val_loss: 0.5288\n",
      "Epoch 12/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5257 - val_loss: 0.5278\n",
      "Epoch 13/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5240 - val_loss: 0.5261\n",
      "Epoch 14/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5206 - val_loss: 0.5219\n",
      "Epoch 15/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5175 - val_loss: 0.5191\n",
      "Epoch 16/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5149 - val_loss: 0.5163\n",
      "Epoch 17/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5118 - val_loss: 0.5138\n",
      "Epoch 18/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5104 - val_loss: 0.5122\n",
      "Epoch 19/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5082 - val_loss: 0.5081\n",
      "Epoch 20/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5065 - val_loss: 0.5065\n",
      "Epoch 21/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5045 - val_loss: 0.5078\n",
      "Epoch 22/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5027 - val_loss: 0.5030\n",
      "Epoch 23/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5014 - val_loss: 0.5022\n",
      "Epoch 24/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.5000 - val_loss: 0.5010\n",
      "Epoch 25/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4970 - val_loss: 0.4979\n",
      "Epoch 26/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4959 - val_loss: 0.4949\n",
      "Epoch 27/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4942 - val_loss: 0.4944\n",
      "Epoch 28/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4925 - val_loss: 0.4918\n",
      "Epoch 29/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4913 - val_loss: 0.4912\n",
      "Epoch 30/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4890 - val_loss: 0.4894\n",
      "Epoch 31/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4879 - val_loss: 0.4886\n",
      "Epoch 32/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4854 - val_loss: 0.4872\n",
      "Epoch 33/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4844 - val_loss: 0.4864\n",
      "Epoch 34/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4838 - val_loss: 0.4846\n",
      "Epoch 35/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4828 - val_loss: 0.4837\n",
      "Epoch 36/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4812 - val_loss: 0.4824\n",
      "Epoch 37/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4799 - val_loss: 0.4813\n",
      "Epoch 38/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4790 - val_loss: 0.4807\n",
      "Epoch 39/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4776 - val_loss: 0.4800\n",
      "Epoch 40/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4764 - val_loss: 0.4783\n",
      "Epoch 41/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4756 - val_loss: 0.4779\n",
      "Epoch 42/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4747 - val_loss: 0.4768\n",
      "Epoch 43/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4736 - val_loss: 0.4764\n",
      "Epoch 44/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4725 - val_loss: 0.4752\n",
      "Epoch 45/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4715 - val_loss: 0.4743\n",
      "Epoch 46/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4706 - val_loss: 0.4736\n",
      "Epoch 47/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4690 - val_loss: 0.4712\n",
      "Epoch 48/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4669 - val_loss: 0.4712\n",
      "Epoch 49/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4663 - val_loss: 0.4707\n",
      "Epoch 50/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4654 - val_loss: 0.4698\n",
      "Epoch 51/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4644 - val_loss: 0.4691\n",
      "Epoch 52/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4635 - val_loss: 0.4690\n",
      "Epoch 53/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4624 - val_loss: 0.4688\n",
      "Epoch 54/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4616 - val_loss: 0.4682\n",
      "Epoch 55/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4612 - val_loss: 0.4665\n",
      "Epoch 56/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4601 - val_loss: 0.4661\n",
      "Epoch 57/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4593 - val_loss: 0.4654\n",
      "Epoch 58/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4585 - val_loss: 0.4643\n",
      "Epoch 59/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4578 - val_loss: 0.4637\n",
      "Epoch 60/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4571 - val_loss: 0.4628\n",
      "Epoch 61/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4564 - val_loss: 0.4621\n",
      "Epoch 62/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4558 - val_loss: 0.4616\n",
      "Epoch 63/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4551 - val_loss: 0.4609\n",
      "Epoch 64/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4544 - val_loss: 0.4604\n",
      "Epoch 65/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4538 - val_loss: 0.4598\n",
      "Epoch 66/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4531 - val_loss: 0.4593\n",
      "Epoch 67/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4524 - val_loss: 0.4587\n",
      "Epoch 68/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4518 - val_loss: 0.4581\n",
      "Epoch 69/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4511 - val_loss: 0.4576\n",
      "Epoch 70/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4505 - val_loss: 0.4570\n",
      "Epoch 71/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4499 - val_loss: 0.4564\n",
      "Epoch 72/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4493 - val_loss: 0.4558\n",
      "Epoch 73/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4487 - val_loss: 0.4552\n",
      "Epoch 74/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4480 - val_loss: 0.4549\n",
      "Epoch 75/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4475 - val_loss: 0.4544\n",
      "Epoch 76/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4469 - val_loss: 0.4541\n",
      "Epoch 77/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4464 - val_loss: 0.4538\n",
      "Epoch 78/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4459 - val_loss: 0.4531\n",
      "Epoch 79/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4453 - val_loss: 0.4528\n",
      "Epoch 80/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4448 - val_loss: 0.4522\n",
      "Epoch 81/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4442 - val_loss: 0.4516\n",
      "Epoch 82/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4437 - val_loss: 0.4511\n",
      "Epoch 83/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4431 - val_loss: 0.4507\n",
      "Epoch 84/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4427 - val_loss: 0.4501\n",
      "Epoch 85/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4421 - val_loss: 0.4496\n",
      "Epoch 86/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4417 - val_loss: 0.4491\n",
      "Epoch 87/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4412 - val_loss: 0.4487\n",
      "Epoch 88/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4408 - val_loss: 0.4481\n",
      "Epoch 89/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4403 - val_loss: 0.4475\n",
      "Epoch 90/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4397 - val_loss: 0.4471\n",
      "Epoch 91/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4394 - val_loss: 0.4465\n",
      "Epoch 92/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4388 - val_loss: 0.4460\n",
      "Epoch 93/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4383 - val_loss: 0.4459\n",
      "Epoch 94/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4387 - val_loss: 0.4449\n",
      "Epoch 95/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4382 - val_loss: 0.4445\n",
      "Epoch 96/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4376 - val_loss: 0.4442\n",
      "Epoch 97/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4370 - val_loss: 0.4441\n",
      "Epoch 98/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4364 - val_loss: 0.4434\n",
      "Epoch 99/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4366 - val_loss: 0.4430\n",
      "Epoch 100/100\n",
      "124999/124999 [==============================] - 0s 3us/step - loss: 0.4355 - val_loss: 0.4446\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11cdfcba8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model without latent variable PCA components\n",
    "input_data = Input(shape=(X.shape[1],))\n",
    "layer1 = Dense(200, activation='sigmoid')(input_data)\n",
    "layer2 = Dense(1, activation='sigmoid')(layer1)\n",
    "model = Model(input_data, layer2)\n",
    "\n",
    "model.compile(optimizer='adagrad', \n",
    "                    loss='binary_crossentropy')\n",
    "model.fit(X, y,\n",
    "                epochs=100,\n",
    "                batch_size=5000,\n",
    "                shuffle=False,\n",
    "                validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.80282557739538085, 0.80756646053168424)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test = model.predict(X_test)\n",
    "p_train = model.predict(X)\n",
    "get_acc(p_test,y_test),get_acc(p_train,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [datasci]",
   "language": "python",
   "name": "Python [datasci]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
